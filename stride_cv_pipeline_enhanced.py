#!/usr/bin/env python3
"""
Enhanced Stride Analysis Cross-Validation Pipeline
Subject-wise LOSO êµì°¨ê²€ì¦ ì™„ì „ ê²€ì¦ ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)

ê°œì„ ì‚¬í•­:
- Step 1: ì‹œí€€ìŠ¤ ê¸¸ì´ outlier ê²€ì¶œ, ë³´ì¡° íŠ¹ì§• í†µê³„ ì €ì¥
- Step 2: LOSO êµ¬ì¡° ëª…í™•í™”, 2ì¤‘ ê²€ì¦
- Step 3: foot ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦, ì¶œë ¥ signature ëª…ì‹œ  
- Step 4: dtype ê²€ì¦, ì •ê·œí™” ì ìš© í™•ì¸
"""

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional
import tensorflow as tf

# ë¡œì»¬ ëª¨ë“ˆ import
from stride_data_processor import StrideDataProcessor
from ragged_data_generator import RaggedStrideDataGenerator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedStrideCVPipeline:
    """Enhanced Subject-wise LOSO êµì°¨ê²€ì¦ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, input_dir="stride_train_data", output_dir="stride_train_data_pkl"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.processor = StrideDataProcessor(input_dir, output_dir)
        self.generator = None
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.validation_results = {}
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„ ì €ì¥
        self.sequence_length_stats = {}
        
    def step1_process_data_enhanced(self) -> Dict:
        """
        Step 1: í–¥ìƒëœ ë°ì´í„° ì²˜ë¦¬ (JSON â†’ PKL ë³€í™˜)
        
        ê°œì„ ì‚¬í•­:
        - ì‹œí€€ìŠ¤ ê¸¸ì´ outlier ë¶„ì„ ë° ê¸°ë¡
        - ë³´ì¡° íŠ¹ì§• í†µê³„ë¥¼ global_norm.npzì— í¬í•¨
        """
        print("\n" + "="*80)
        print("STEP 1: í–¥ìƒëœ ë°ì´í„° ì²˜ë¦¬ (JSON â†’ PKL ë³€í™˜)")
        print("="*80)
        
        # 1. ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬ ë¶„ì„
        sequence_lengths = self._analyze_sequence_lengths()
        
        # 2. ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰
        results = self.processor.process_all()
        
        # 3. í–¥ìƒëœ ì •ê·œí™” í†µê³„ ì €ì¥ (ë³´ì¡° íŠ¹ì§• í¬í•¨)
        self._save_enhanced_normalization_stats()
        
        # ê²°ê³¼ ê²€ì¦
        if results['stats']['valid_sessions'] == 0:
            raise ValueError("No valid sessions processed!")
        
        print(f"âœ… Step 1 ì™„ë£Œ: {results['stats']['valid_sessions']} ì„¸ì…˜ ì²˜ë¦¬ë¨")
        print(f"ğŸ“Š ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„: {self.sequence_length_stats}")
        
        self.validation_results['step1'] = {
            'status': 'completed',
            'valid_sessions': results['stats']['valid_sessions'],
            'total_cycles': results['stats']['valid_cycles'],
            'pkl_files_created': results['stats']['valid_sessions'],
            'sequence_length_stats': self.sequence_length_stats
        }
        
        return results    
    def _analyze_sequence_lengths(self) -> List[int]:
        """ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬ ë¶„ì„ ë° outlier ê²€ì¶œ"""
        print("ğŸ“Š ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬ ë¶„ì„ ì¤‘...")
        
        all_lengths = []
        
        # ëª¨ë“  JSON íŒŒì¼ì—ì„œ ì‹œí€€ìŠ¤ ê¸¸ì´ ìˆ˜ì§‘
        for session_dir in self.input_dir.iterdir():
            if not session_dir.is_dir():
                continue
                
            json_file = session_dir / f"{session_dir.name}_Cycles.json"
            if not json_file.exists():
                continue
                
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    cycles_data = json.load(f)
                
                for cycle in cycles_data:
                    if 'sequence' in cycle and cycle['sequence']:
                        all_lengths.append(len(cycle['sequence']))
                        
            except Exception as e:
                logger.warning(f"Error reading {json_file}: {e}")
                continue
        
        if not all_lengths:
            logger.warning("No sequences found for length analysis")
            return []
        
        # í†µê³„ ê³„ì‚°
        lengths_array = np.array(all_lengths)
        self.sequence_length_stats = {
            'total_sequences': len(all_lengths),
            'min_length': int(np.min(lengths_array)),
            'max_length': int(np.max(lengths_array)),
            'mean_length': float(np.mean(lengths_array)),
            'median_length': float(np.median(lengths_array)),
            'p95_length': float(np.percentile(lengths_array, 95)),
            'p99_length': float(np.percentile(lengths_array, 99)),
            'std_length': float(np.std(lengths_array))
        }
        
        # Outlier ì„ê³„ê°’ ê³„ì‚° (p99 + 5)
        outlier_threshold = self.sequence_length_stats['p99_length'] + 5
        outliers = lengths_array[lengths_array > outlier_threshold]
        
        self.sequence_length_stats['outlier_threshold'] = outlier_threshold
        self.sequence_length_stats['n_outliers'] = len(outliers)
        
        print(f"  ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ ë²”ìœ„: {self.sequence_length_stats['min_length']} ~ {self.sequence_length_stats['max_length']}")
        print(f"  ğŸ“ˆ í‰ê· : {self.sequence_length_stats['mean_length']:.1f}, ì¤‘ì•™ê°’: {self.sequence_length_stats['median_length']:.1f}")
        print(f"  ğŸ¯ P95: {self.sequence_length_stats['p95_length']:.1f}, P99: {self.sequence_length_stats['p99_length']:.1f}")
        print(f"  âš ï¸  Outlier ì„ê³„ê°’ (P99+5): {outlier_threshold:.1f}, ê°œìˆ˜: {len(outliers)}")
        
        # í˜„ì¬ í•„í„°ë§ ê¸°ì¤€ (15~100) í‰ê°€
        current_min, current_max = 15, 100
        filtered_out = np.sum((lengths_array < current_min) | (lengths_array > current_max))
        print(f"  ğŸ” í˜„ì¬ í•„í„° (15~100)ë¡œ ì œê±°ë˜ëŠ” ì‹œí€€ìŠ¤: {filtered_out}/{len(all_lengths)} ({filtered_out/len(all_lengths)*100:.1f}%)")
        
        return all_lengths
    
    def _save_enhanced_normalization_stats(self):
        """í–¥ìƒëœ ì •ê·œí™” í†µê³„ ì €ì¥ (ë³´ì¡° íŠ¹ì§• í¬í•¨)"""
        print("ğŸ’¾ í–¥ìƒëœ ì •ê·œí™” í†µê³„ ì €ì¥ ì¤‘...")
        
        # ê¸°ì¡´ ì‹œí€€ìŠ¤ ì •ê·œí™” í†µê³„ ë¡œë“œ (metadata í´ë”ì—ì„œ)
        metadata_dir = Path("metadata")
        global_norm_file = metadata_dir / 'global_norm.npz'
        
        if not global_norm_file.exists():
            logger.warning(f"global_norm.npz not found in {metadata_dir}, skipping enhanced stats")
            return
        
        existing_stats = np.load(global_norm_file)
        
        # ë³´ì¡° íŠ¹ì§• í†µê³„ ê³„ì‚°
        all_stride_times = []
        all_heights = []
        
        for pkl_file in self.output_dir.glob("*.pkl"):
            try:
                with open(pkl_file, 'rb') as f:
                    data = pickle.load(f)
                
                for cycle in data['cycles']:
                    stride_time = cycle.get('stride_time', 0.0)
                    height = cycle.get('height', 0.0)
                    
                    if stride_time > 0:
                        all_stride_times.append(stride_time)
                    if height > 0:
                        all_heights.append(height)
                        
            except Exception as e:
                logger.warning(f"Error reading {pkl_file}: {e}")
                continue
        
        # í–¥ìƒëœ í†µê³„ ì €ì¥
        enhanced_stats = {
            # ê¸°ì¡´ ì‹œí€€ìŠ¤ í†µê³„
            'sequence_mean': existing_stats['mean'],
            'sequence_std': existing_stats['std'],
            'sequence_n_samples': existing_stats['n_samples'],
            
            # ë³´ì¡° íŠ¹ì§• í†µê³„
            'stride_time_mean': np.mean(all_stride_times) if all_stride_times else 0.0,
            'stride_time_std': np.std(all_stride_times) if all_stride_times else 1.0,
            'stride_time_n_samples': len(all_stride_times),
            
            'height_mean': np.mean(all_heights) if all_heights else 0.0,
            'height_std': np.std(all_heights) if all_heights else 1.0,
            'height_n_samples': len(all_heights),
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„
            'sequence_length_stats': self.sequence_length_stats
        }
        
        enhanced_norm_file = metadata_dir / 'global_norm_enhanced.npz'
        np.savez(enhanced_norm_file, **enhanced_stats)
        print(f"âœ… í–¥ìƒëœ ì •ê·œí™” í†µê³„ ì €ì¥ ì™„ë£Œ: stride_time({len(all_stride_times)}), height({len(all_heights)}) -> {enhanced_norm_file}")    
    def step2_validate_cv_splits_enhanced(self) -> Dict:
        """
        Step 2: í–¥ìƒëœ êµì°¨ê²€ì¦ Split ê²€ì¦
        
        ê°œì„ ì‚¬í•­:
        - LOSO êµ¬ì¡° ëª…í™•í™” (val â‰¡ test)
        - 2ì¤‘ ê²€ì¦ ê°•í™”
        """
        print("\n" + "="*80)
        print("STEP 2: í–¥ìƒëœ êµì°¨ê²€ì¦ Split ê²€ì¦")
        print("="*80)
        
        # CV splits íŒŒì¼ í™•ì¸ (metadata í´ë”ì—ì„œ)
        metadata_dir = Path("metadata")
        cv_splits_file = metadata_dir / 'cv_splits.json'
        file_index_file = metadata_dir / 'file_index.csv'
        
        if not cv_splits_file.exists():
            raise FileNotFoundError(f"cv_splits.json not found in {metadata_dir}!")
        
        if not file_index_file.exists():
            raise FileNotFoundError(f"file_index.csv not found in {metadata_dir}!")
        
        # CV splits ë¡œë“œ ë° ê²€ì¦
        with open(cv_splits_file, 'r', encoding='utf-8') as f:
            cv_splits = json.load(f)
        
        df = pd.read_csv(file_index_file)
        
        print("ğŸ“‹ LOSO (Leave-One-Subject-Out) êµ¬ì¡° ê²€ì¦:")
        print("   âœ“ LOSOì—ì„œëŠ” validation â‰¡ test (í•œ ëª…ì˜ subject ì „ì²´)")
        print("   âœ“ ê° foldì—ì„œ test subjectì˜ ëª¨ë“  ì„¸ì…˜ì´ validation set")
        print("   âœ“ ë‚˜ë¨¸ì§€ ëª¨ë“  subjectì˜ ì„¸ì…˜ë“¤ì´ training set")
        
        # CV splits ê²€ì¦
        validation_result = self._validate_cv_splits_enhanced(cv_splits, df)
        
        if not validation_result['overall_valid']:
            raise ValueError(f"CV splits ê²€ì¦ ì‹¤íŒ¨: {validation_result.get('overlap_issues', 'Unknown error')}")
        
        print(f"âœ… Step 2 ì™„ë£Œ: {validation_result['n_folds']}ê°œ fold, {validation_result['n_subjects']}ëª… subject ê²€ì¦ í†µê³¼")
        
        self.validation_results['step2'] = {
            'status': 'completed',
            'n_folds': validation_result['n_folds'],
            'n_subjects': validation_result['n_subjects'],
            'unique_subjects': validation_result['unique_subjects'],
            'subject_coverage': validation_result['subject_coverage'],
            'file_coverage': validation_result['file_coverage'],
            'fold_details': validation_result['fold_details']
        }
        
        return {
            'cv_splits_valid': True,
            'n_folds': validation_result['n_folds'],
            'unique_subjects': validation_result['unique_subjects'],
            'validation_details': validation_result
        }
    
    def _validate_cv_splits_enhanced(self, cv_splits: List[Dict], df: pd.DataFrame) -> Dict:
        """í–¥ìƒëœ CV splits ê²€ì¦"""
        results = {
            'n_folds': len(cv_splits),
            'unique_subjects': [],
            'fold_details': [],
            'subject_coverage': {},
            'file_coverage': {},
            'overlap_issues': []
        }
        
        all_subjects = set()
        all_files_in_splits = set()
        
        for i, fold in enumerate(cv_splits):
            fold_num = i + 1
            test_subject = fold['test_subject']
            train_subjects = fold['train_subjects']
            train_files = fold['train_files']
            val_files = fold['val_files']
            
            # Subject ìˆ˜ì§‘
            all_subjects.add(test_subject)
            all_subjects.update(train_subjects)
            
            # íŒŒì¼ ìˆ˜ì§‘
            all_files_in_splits.update(train_files)
            all_files_in_splits.update(val_files)
            
            # LOSO ì›ì¹™ ê²€ì¦: trainê³¼ valì€ ì™„ì „íˆ ë‹¤ë¥¸ subjectì—¬ì•¼ í•¨
            train_file_subjects = set()
            val_file_subjects = set()
            
            for f in train_files:
                subject = f.split('T')[0]  # S01T01R01_Cycles.pkl -> S01
                train_file_subjects.add(subject)
            
            for f in val_files:
                subject = f.split('T')[0]  # S01T01R01_Cycles.pkl -> S01
                val_file_subjects.add(subject)
            
            # **í•µì‹¬ ìˆ˜ì •**: íŒŒì¼ level overlapì´ ì•„ë‹Œ subject level overlap ê²€ì‚¬
            subject_overlap = train_file_subjects & val_file_subjects
            train_val_overlap = len(subject_overlap) > 0
            
            if train_val_overlap:
                overlap_detail = f"Fold {fold_num}: Subject overlap detected: {subject_overlap}"
                results['overlap_issues'].append(overlap_detail)
                print(f"  âŒ {overlap_detail}")
            
            # Val subjectê°€ test subjectì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (LOSO íŠ¹ì„±)
            val_subjects_expected = {test_subject}
            val_subjects_actual = val_file_subjects
            test_val_consistency = val_subjects_actual == val_subjects_expected
            
            if not test_val_consistency:
                issue = f"Fold {fold_num}: Val subjects {val_subjects_actual} != Test subject {val_subjects_expected}"
                results['overlap_issues'].append(issue)
                print(f"  âŒ {issue}")
            
            # Train subjects í™•ì¸
            train_subjects_from_files = train_file_subjects
            train_subjects_expected = set(train_subjects)
            train_consistency = train_subjects_from_files == train_subjects_expected
            
            if not train_consistency:
                issue = f"Fold {fold_num}: Train subjects mismatch. Expected: {train_subjects_expected}, Got: {train_subjects_from_files}"
                results['overlap_issues'].append(issue)
                print(f"  âŒ {issue}")
            
            fold_detail = {
                'fold': fold_num,
                'test_subject': test_subject,
                'train_subjects': train_subjects,
                'n_train_files': len(train_files),
                'n_val_files': len(val_files),
                'train_file_subjects': sorted(train_file_subjects),
                'val_file_subjects': sorted(val_file_subjects),
                'subject_overlap': train_val_overlap,
                'test_val_consistency': test_val_consistency,
                'train_consistency': train_consistency
            }
            
            results['fold_details'].append(fold_detail)
            
            print(f"  ğŸ“ Fold {fold_num}: Test={test_subject}, Train={len(train_files)}, Val={len(val_files)}")
            print(f"     Train Subjects: {sorted(train_file_subjects)}")
            print(f"     Val Subjects: {sorted(val_file_subjects)}")
            print(f"     Subject Overlap: {'âŒ YES' if train_val_overlap else 'âœ… NO'}")
            print(f"     Test-Val Consistency: {'âœ… YES' if test_val_consistency else 'âŒ NO'}")
        
        # ì „ì²´ ê²€ì¦
        results['unique_subjects'] = sorted(all_subjects)
        results['n_subjects'] = len(all_subjects)
        
        # Subject coverage í™•ì¸
        for subject in all_subjects:
            subject_files = df[df['subject'] == subject]['file_name'].tolist()
            results['subject_coverage'][subject] = len(subject_files)
        
        # File coverage í™•ì¸
        all_files_in_df = set(df['file_name'].tolist())
        missing_in_splits = all_files_in_df - all_files_in_splits
        extra_in_splits = all_files_in_splits - all_files_in_df
        
        results['file_coverage'] = {
            'total_in_df': len(all_files_in_df),
            'total_in_splits': len(all_files_in_splits),
            'missing_in_splits': sorted(missing_in_splits),
            'extra_in_splits': sorted(extra_in_splits),
            'coverage_complete': len(missing_in_splits) == 0 and len(extra_in_splits) == 0
        }
        
        # ì „ì²´ ìœ íš¨ì„±
        all_valid = (
            len(results['overlap_issues']) == 0 and
            results['file_coverage']['coverage_complete'] and
            len(all_subjects) >= 2  # ìµœì†Œ 2ëª…ì˜ subject í•„ìš”
        )
        
        results['overall_valid'] = all_valid
        
        if not all_valid:
            print(f"\nâŒ CV Splits ê²€ì¦ ì‹¤íŒ¨:")
            for issue in results['overlap_issues']:
                print(f"  - {issue}")
            if not results['file_coverage']['coverage_complete']:
                print(f"  - File coverage incomplete")
                if missing_in_splits:
                    print(f"    Missing: {len(missing_in_splits)} files")
                if extra_in_splits:
                    print(f"    Extra: {len(extra_in_splits)} files")
        else:
            print(f"\nâœ… CV Splits ê²€ì¦ í†µê³¼: {len(cv_splits)}ê°œ fold, {len(all_subjects)}ëª… subject")
        
        return results    
    def step3_create_data_generator_enhanced(self) -> RaggedStrideDataGenerator:
        """
        Step 3: í–¥ìƒëœ RaggedTensor ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
        
        ê°œì„ ì‚¬í•­:
        - foot ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦
        - ì¶œë ¥ signature ëª…ì‹œ
        """
        print("\n" + "="*80)
        print("STEP 3: í–¥ìƒëœ RaggedTensor ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±")
        print("="*80)
        
        # ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”
        self.generator = RaggedStrideDataGenerator()
        
        # ê¸°ë³¸ ê²€ì¦
        if self.generator.cv_splits is None:
            raise ValueError("Failed to load CV splits!")
        
        if self.generator.normalization_stats is None:
            raise ValueError("Failed to load normalization stats!")
        
        # foot ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦
        foot_mapping_check = self._validate_foot_mapping()
        
        # ì¶œë ¥ signature ëª…ì‹œ
        output_signature = self._define_output_signature()
        
        print("âœ… Step 3 ì™„ë£Œ: í–¥ìƒëœ RaggedTensor ë°ì´í„° ì œë„ˆë ˆì´í„° ì¤€ë¹„ë¨")
        print(f"ğŸ¦¶ Foot ë§¤í•‘ ê²€ì¦: {foot_mapping_check}")
        print(f"ğŸ“ ì¶œë ¥ Signature: {output_signature}")
        
        self.validation_results['step3'] = {
            'status': 'completed',
            'cv_splits_loaded': self.generator.cv_splits is not None,
            'normalization_loaded': self.generator.normalization_stats is not None,
            'auxiliary_stats_loaded': self.generator.auxiliary_stats is not None,
            'foot_mapping_validation': foot_mapping_check,
            'output_signature': output_signature
        }
        
        return self.generator
    
    def _validate_foot_mapping(self) -> Dict:
        """foot â†’ ì •ìˆ˜ ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦"""
        print("ğŸ¦¶ Foot ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦ ì¤‘...")
        
        foot_values = {'left': 0, 'right': 0, 'unknown': 0, 'other': 0}
        foot_examples = {}
        
        # ì¼ë¶€ PKL íŒŒì¼ì—ì„œ foot ê°’ë“¤ ìˆ˜ì§‘
        pkl_files = list(self.generator.pkl_dir.glob("*.pkl"))[:10]  # ì²« 10ê°œë§Œ ìƒ˜í”Œë§
        
        for pkl_file in pkl_files:
            try:
                with open(pkl_file, 'rb') as f:
                    data = pickle.load(f)
                
                for cycle in data['cycles']:
                    foot = cycle.get('foot', 'unknown')
                    foot_lower = foot.lower()
                    
                    if foot_lower == 'left':
                        foot_values['left'] += 1
                    elif foot_lower == 'right':
                        foot_values['right'] += 1
                    elif foot_lower == 'unknown':
                        foot_values['unknown'] += 1
                    else:
                        foot_values['other'] += 1
                        if foot not in foot_examples:
                            foot_examples[foot] = 0
                        foot_examples[foot] += 1
                        
            except Exception as e:
                logger.warning(f"Error reading {pkl_file}: {e}")
                continue
        
        # ë§¤í•‘ ê·œì¹™ í™•ì¸
        expected_mapping = {'left': 0.0, 'right': 1.0, 'unknown': -1.0}
        
        mapping_result = {
            'foot_distribution': foot_values,
            'unexpected_values': foot_examples,
            'expected_mapping': expected_mapping,
            'is_consistent': len(foot_examples) == 0,
            'total_samples_checked': sum(foot_values.values())
        }
        
        if foot_examples:
            print(f"  âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ foot ê°’ë“¤: {foot_examples}")
        else:
            print("  âœ… ëª¨ë“  foot ê°’ì´ ì˜ˆìƒëœ ë²”ìœ„ ë‚´ (left/right/unknown)")
        
        print(f"  ğŸ“Š Foot ë¶„í¬: {foot_values}")
        
        return mapping_result
    
    def _define_output_signature(self) -> Dict:
        """ì¶œë ¥ signature ëª…ì‹œ"""
        signature = {
            'input': {
                'sequences': 'tf.RaggedTensor, shape=(batch_size, None, 6), dtype=tf.float32',
                'auxiliary': 'tf.Tensor, shape=(batch_size, 3), dtype=tf.float32',
                'auxiliary_features': ['stride_time (normalized)', 'height (normalized)', 'foot_id (0/1/-1)']
            },
            'output': {
                'labels': 'tf.Tensor, shape=(batch_size,), dtype=tf.float32',
                'target': 'stride_length (meters)'
            },
            'batch_structure': '((sequences, auxiliary), labels)',
            'normalization': {
                'sequences': 'z-score (6-axis IMU)',
                'stride_time': 'z-score',
                'height': 'z-score', 
                'foot_id': 'categorical (no normalization)'
            }
        }
        
        return signature    
    def step4_validate_all_folds_enhanced(self) -> Dict:
        """
        Step 4: í–¥ìƒëœ ëª¨ë“  Fold ë°ì´í„°ì…‹ ìƒì„± ë° ê²€ì¦
        
        ê°œì„ ì‚¬í•­:
        - dtype ê²€ì¦ ê°•í™”
        - ì •ê·œí™” ì ìš© í™•ì¸
        """
        print("\n" + "="*80)
        print("STEP 4: í–¥ìƒëœ ëª¨ë“  Fold ë°ì´í„°ì…‹ ìƒì„± ë° ê²€ì¦")
        print("="*80)
        
        if self.generator is None:
            raise ValueError("Data generator not initialized!")
        
        fold_results = []
        total_train_cycles = 0
        total_val_cycles = 0
        
        # ê° foldë³„ ê²€ì¦
        for fold_idx in range(len(self.generator.cv_splits)):
            print(f"\nğŸ“ Fold {fold_idx + 1} í–¥ìƒëœ ê²€ì¦ ì¤‘...")
            
            try:
                # Dataset ìƒì„±
                train_ds, val_ds, fold_info = self.generator.get_fold_datasets(
                    fold_idx, batch_size=64, normalize=True
                )
                
                # Subject ëˆ„ìˆ˜ ê²€ì¦
                train_subjects = set()
                val_subjects = set()
                
                for metadata in fold_info['train_metadata']:
                    train_subjects.add(metadata['subject'])
                
                for metadata in fold_info['val_metadata']:
                    val_subjects.add(metadata['subject'])
                
                # ëˆ„ìˆ˜ ê²€ì‚¬
                subject_leak = train_subjects & val_subjects
                if subject_leak:
                    raise ValueError(f"Subject leak detected in fold {fold_idx + 1}: {subject_leak}")
                
                # í–¥ìƒëœ ë°°ì¹˜ í˜•íƒœ ê²€ì¦
                batch_validation = self._validate_batch_format_enhanced(train_ds, val_ds, fold_idx + 1)
                
                fold_result = {
                    'fold': fold_idx + 1,
                    'test_subject': fold_info['test_subject'],
                    'train_subjects': fold_info['train_subjects'],
                    'n_train_cycles': fold_info['n_train_cycles'],
                    'n_val_cycles': fold_info['n_val_cycles'],
                    'train_subjects_actual': sorted(train_subjects),
                    'val_subjects_actual': sorted(val_subjects),
                    'subject_leak': len(subject_leak) == 0,
                    'batch_validation': batch_validation
                }
                
                fold_results.append(fold_result)
                total_train_cycles += fold_info['n_train_cycles']
                total_val_cycles += fold_info['n_val_cycles']
                
                print(f"  âœ… Fold {fold_idx + 1}: Train={fold_info['n_train_cycles']:,}, Val={fold_info['n_val_cycles']:,}")
                print(f"     Test Subject: {fold_info['test_subject']}")
                print(f"     Subject Leak: {'âŒ DETECTED' if subject_leak else 'âœ… NONE'}")
                print(f"     Batch Validation: {'âœ… PASS' if batch_validation['overall_valid'] else 'âŒ FAIL'}")
                
            except Exception as e:
                print(f"  âŒ Fold {fold_idx + 1} ì‹¤íŒ¨: {str(e)}")
                raise
        
        print(f"\nâœ… Step 4 ì™„ë£Œ: ëª¨ë“  {len(fold_results)}ê°œ fold í–¥ìƒëœ ê²€ì¦ í†µê³¼")
        print(f"   ì´ Train Cycles: {total_train_cycles:,}")
        print(f"   ì´ Val Cycles: {total_val_cycles:,}")
        
        self.validation_results['step4'] = {
            'status': 'completed',
            'n_folds_validated': len(fold_results),
            'total_train_cycles': total_train_cycles,
            'total_val_cycles': total_val_cycles,
            'fold_results': fold_results
        }
        
        return {
            'fold_results': fold_results,
            'total_train_cycles': total_train_cycles,
            'total_val_cycles': total_val_cycles
        }
    
    def _validate_batch_format_enhanced(self, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, fold_num: int) -> Dict:
        """í–¥ìƒëœ ë°°ì¹˜ í˜•íƒœ ê²€ì¦"""
        validation_result = {'train': {}, 'val': {}, 'overall_valid': True}
        
        # Train dataset ê²€ì¦
        train_valid = self._validate_single_dataset_enhanced(train_ds, "Train")
        validation_result['train'] = train_valid
        
        # Validation dataset ê²€ì¦
        val_valid = self._validate_single_dataset_enhanced(val_ds, "Validation")
        validation_result['val'] = val_valid
        
        # ì „ì²´ ìœ íš¨ì„±
        validation_result['overall_valid'] = train_valid['valid'] and val_valid['valid']
        
        return validation_result    
    def _validate_single_dataset_enhanced(self, dataset: tf.data.Dataset, dataset_name: str) -> Dict:
        """ë‹¨ì¼ ë°ì´í„°ì…‹ í–¥ìƒëœ ê²€ì¦"""
        try:
            batch_count = 0
            for batch in dataset.take(3):  # ì²« 3ê°œ ë°°ì¹˜ ê²€ì¦
                batch_count += 1
                (sequences, auxiliary), labels = batch
                
                # ê¸°ë³¸ í˜•íƒœ ê²€ì¦
                basic_validation = {
                    'sequences_shape': str(sequences.shape),
                    'sequences_dtype': str(sequences.dtype),
                    'auxiliary_shape': str(auxiliary.shape),
                    'auxiliary_dtype': str(auxiliary.dtype),
                    'labels_shape': str(labels.shape),
                    'labels_dtype': str(labels.dtype),
                    'is_ragged': isinstance(sequences, tf.RaggedTensor),
                    'batch_size': sequences.shape[0]
                }
                
                # dtype ê²€ì¦ ê°•í™”
                dtype_validation = {
                    'sequences_is_float32': sequences.dtype == tf.float32,
                    'auxiliary_is_float32': auxiliary.dtype == tf.float32,
                    'labels_is_float32': labels.dtype == tf.float32
                }
                
                # ìœ í•œê°’ ê²€ì¦
                if isinstance(sequences, tf.RaggedTensor):
                    sequences_finite = tf.reduce_all(tf.math.is_finite(sequences.flat_values)).numpy()
                else:
                    sequences_finite = tf.reduce_all(tf.math.is_finite(sequences)).numpy()
                
                auxiliary_finite = tf.reduce_all(tf.math.is_finite(auxiliary)).numpy()
                labels_finite = tf.reduce_all(tf.math.is_finite(labels)).numpy()
                
                dtype_validation.update({
                    'sequences_finite': sequences_finite,
                    'auxiliary_finite': auxiliary_finite,
                    'labels_finite': labels_finite
                })
                
                # ì •ê·œí™” ì ìš© í™•ì¸
                normalization_validation = self._check_normalization_applied(sequences, auxiliary)
                
                # Shape ê²€ì¦
                shape_validation = {
                    'auxiliary_3_features': auxiliary.shape[-1] == 3,
                    'labels_1d': len(labels.shape) == 1,
                    'batch_size_consistent': (sequences.shape[0] == auxiliary.shape[0] == labels.shape[0])
                }
                
                if isinstance(sequences, tf.RaggedTensor):
                    shape_validation['sequences_last_dim_6'] = sequences.shape[-1] == 6
                else:
                    shape_validation['sequences_last_dim_6'] = sequences.shape[-1] == 6
                
                # ì „ì²´ ìœ íš¨ì„±
                all_valid = (
                    dtype_validation['sequences_is_float32'] and
                    dtype_validation['auxiliary_is_float32'] and 
                    dtype_validation['labels_is_float32'] and
                    dtype_validation['sequences_finite'] and
                    dtype_validation['auxiliary_finite'] and
                    dtype_validation['labels_finite'] and
                    shape_validation['auxiliary_3_features'] and
                    shape_validation['labels_1d'] and
                    shape_validation['batch_size_consistent'] and
                    shape_validation['sequences_last_dim_6'] and
                    normalization_validation['sequences_normalized'] and
                    normalization_validation['auxiliary_normalized']
                )
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ìì„¸í•œ ì •ë³´ ë°˜í™˜
                if batch_count == 1:
                    result = {
                        'valid': all_valid,
                        'basic': basic_validation,
                        'dtype': dtype_validation,
                        'shape': shape_validation,
                        'normalization': normalization_validation
                    }
                    
                    # ì‹¤íŒ¨í•œ ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                    if not all_valid:
                        print(f"    ğŸ” {dataset_name} ë°°ì¹˜ ê²€ì¦ ì‹¤íŒ¨ ìƒì„¸:")
                        if not dtype_validation['sequences_is_float32']:
                            print(f"      - sequences dtype: {sequences.dtype} (expected: tf.float32)")
                        if not dtype_validation['auxiliary_is_float32']:
                            print(f"      - auxiliary dtype: {auxiliary.dtype} (expected: tf.float32)")
                        if not dtype_validation['labels_is_float32']:
                            print(f"      - labels dtype: {labels.dtype} (expected: tf.float32)")
                        if not dtype_validation['sequences_finite']:
                            print(f"      - sequences contains non-finite values")
                        if not dtype_validation['auxiliary_finite']:
                            print(f"      - auxiliary contains non-finite values")
                        if not dtype_validation['labels_finite']:
                            print(f"      - labels contains non-finite values")
                        if not shape_validation['auxiliary_3_features']:
                            print(f"      - auxiliary shape: {auxiliary.shape} (expected: (batch, 3))")
                        if not shape_validation['labels_1d']:
                            print(f"      - labels shape: {labels.shape} (expected: (batch,))")
                        if not shape_validation['sequences_last_dim_6']:
                            print(f"      - sequences last dim: {sequences.shape[-1]} (expected: 6)")
                        if not normalization_validation['sequences_normalized']:
                            print(f"      - sequences not normalized properly")
                            print(f"        Mean: {normalization_validation['sequences_stats']['mean'][:3]}...")
                            print(f"        Var: {normalization_validation['sequences_stats']['var'][:3]}...")
                        if not normalization_validation['auxiliary_normalized']:
                            print(f"      - auxiliary not normalized properly")
                            stride_stats = normalization_validation['stride_time_stats']
                            height_stats = normalization_validation['height_stats']
                            print(f"        stride_time - mean: {stride_stats['mean']:.3f}, var: {stride_stats['var']:.3f}")
                            print(f"        height - mean: {height_stats['mean']:.3f}, var: {height_stats['var']:.3f}")
                    
                    return result
                
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ì€ ìœ íš¨ì„±ë§Œ í™•ì¸
                if not all_valid:
                    return {
                        'valid': False,
                        'error': f"Batch {batch_count} validation failed"
                    }
            
            # ëª¨ë“  ë°°ì¹˜ê°€ í†µê³¼í•˜ë©´ ì„±ê³µ
            return {'valid': True, 'batches_checked': batch_count}
                
        except Exception as e:
            print(f"    âŒ {dataset_name} ë°°ì¹˜ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'valid': False,
                'error': str(e)
            }
    
    def _check_normalization_applied(self, sequences: tf.RaggedTensor, auxiliary: tf.Tensor) -> Dict:
        """ì •ê·œí™” ì ìš© í™•ì¸"""
        
        # ì‹œí€€ìŠ¤ ì •ê·œí™” í™•ì¸ (í‰ê· â‰ˆ0, ë¶„ì‚°â‰ˆ1)
        if isinstance(sequences, tf.RaggedTensor):
            seq_flat = sequences.flat_values
        else:
            seq_flat = tf.reshape(sequences, [-1, sequences.shape[-1]])
            
        seq_mean = tf.reduce_mean(seq_flat, axis=0)
        seq_var = tf.reduce_mean(tf.square(seq_flat - seq_mean), axis=0)
        
        # ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì ìš©
        seq_mean_close_to_zero = tf.reduce_all(tf.abs(seq_mean) < 0.3).numpy()
        seq_var_close_to_one = tf.reduce_all(tf.abs(seq_var - 1.0) < 0.5).numpy()
        
        # ë³´ì¡° íŠ¹ì§• ì •ê·œí™” í™•ì¸ (stride_time, heightë§Œ)
        aux_stride_time = auxiliary[:, 0]  # stride_time
        aux_height = auxiliary[:, 1]       # height
        aux_foot_id = auxiliary[:, 2]      # foot_id (ì •ê·œí™” ì•ˆë¨)
        
        stride_time_mean = tf.reduce_mean(aux_stride_time)
        stride_time_var = tf.reduce_mean(tf.square(aux_stride_time - stride_time_mean))
        
        height_mean = tf.reduce_mean(aux_height)
        height_var = tf.reduce_mean(tf.square(aux_height - height_mean))
        
        # ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì ìš©
        stride_time_normalized = (tf.abs(stride_time_mean) < 0.3).numpy() and (tf.abs(stride_time_var - 1.0) < 0.5).numpy()
        height_normalized = (tf.abs(height_mean) < 0.3).numpy() and (tf.abs(height_var - 1.0) < 0.5).numpy()
        
        # foot_idëŠ” ì •ê·œí™”ë˜ì§€ ì•Šì•„ì•¼ í•¨ (0, 1, -1 ê°’)
        foot_unique_values = tf.unique(aux_foot_id)[0]
        # foot_id ê°’ë“¤ì´ {-1, 0, 1} ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
        foot_values_valid = tf.reduce_all(
            tf.logical_and(
                foot_unique_values >= -1.0,
                foot_unique_values <= 1.0
            )
        ).numpy()
        
        # foot_idê°€ ëŒ€ëµ 0, 1, -1 ê·¼ì²˜ì˜ ê°’ë“¤ì¸ì§€ í™•ì¸ (ì™„ì „ ì •ìˆ˜ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ)
        foot_id_not_normalized = foot_values_valid
        
        return {
            'sequences_normalized': seq_mean_close_to_zero and seq_var_close_to_one,
            'sequences_stats': {
                'mean': seq_mean.numpy().tolist(),
                'var': seq_var.numpy().tolist(),
                'mean_max_abs': float(tf.reduce_max(tf.abs(seq_mean)).numpy()),
                'var_max_deviation': float(tf.reduce_max(tf.abs(seq_var - 1.0)).numpy())
            },
            'auxiliary_normalized': stride_time_normalized and height_normalized,
            'stride_time_stats': {
                'mean': stride_time_mean.numpy().item(),
                'var': stride_time_var.numpy().item(),
                'normalized': stride_time_normalized
            },
            'height_stats': {
                'mean': height_mean.numpy().item(),
                'var': height_var.numpy().item(), 
                'normalized': height_normalized
            },
            'foot_id_stats': {
                'unique_values': foot_unique_values.numpy().tolist(),
                'not_normalized': foot_id_not_normalized,
                'values_valid': foot_values_valid,
                'mean': float(tf.reduce_mean(aux_foot_id).numpy()),
                'min_max': [float(tf.reduce_min(aux_foot_id).numpy()), float(tf.reduce_max(aux_foot_id).numpy())]
            }
        }
    
    def run_enhanced_pipeline(self) -> Dict:
        """í–¥ìƒëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "ğŸš€" + "="*78 + "ğŸš€")
        print("ENHANCED STRIDE ANALYSIS CV PIPELINE - ì™„ì „ ê²€ì¦ ì‹œì‘")
        print("ğŸš€" + "="*78 + "ğŸš€")
        
        try:
            # Step 1: í–¥ìƒëœ ë°ì´í„° ì²˜ë¦¬
            step1_results = self.step1_process_data_enhanced()
            
            # Step 2: í–¥ìƒëœ CV splits ê²€ì¦
            step2_results = self.step2_validate_cv_splits_enhanced()
            
            # Step 3: í–¥ìƒëœ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
            step3_results = self.step3_create_data_generator_enhanced()
            
            # Step 4: í–¥ìƒëœ ëª¨ë“  fold ê²€ì¦
            step4_results = self.step4_validate_all_folds_enhanced()
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            self.print_enhanced_summary()
            
            return {
                'status': 'SUCCESS',
                'validation_results': self.validation_results,
                'step_results': {
                    'step1': step1_results,
                    'step2': step2_results,
                    'step3': step3_results,
                    'step4': step4_results
                }
            }
            
        except Exception as e:
            print(f"\nâŒ í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            
            return {
                'status': 'FAILED',
                'error': str(e),
                'validation_results': self.validation_results
            }
    
    def print_enhanced_summary(self):
        """í–¥ìƒëœ ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "ğŸ‰" + "="*78 + "ğŸ‰")
        print("ENHANCED STRIDE ANALYSIS CV PIPELINE - ì™„ë£Œ ìš”ì•½")
        print("ğŸ‰" + "="*78 + "ğŸ‰")
        
        # Stepë³„ ìƒíƒœ í™•ì¸
        for step_name, step_result in self.validation_results.items():
            status = "âœ… ì™„ë£Œ" if step_result['status'] == 'completed' else "âŒ ì‹¤íŒ¨"
            print(f"{step_name.upper()}: {status}")
        
        # í•µì‹¬ ì§€í‘œ ì¶œë ¥
        if 'step4' in self.validation_results:
            step4 = self.validation_results['step4']
            print(f"\nğŸ“Š í•µì‹¬ ì§€í‘œ:")
            print(f"  ê²€ì¦ëœ Fold ìˆ˜: {step4['n_folds_validated']}")
            print(f"  ì´ Train Cycles: {step4['total_train_cycles']:,}")
            print(f"  ì´ Val Cycles: {step4['total_val_cycles']:,}")
            
            # Subjectë³„ ë¶„í¬
            if 'step2' in self.validation_results:
                subjects = self.validation_results['step2']['unique_subjects']
                print(f"  Subject ìˆ˜: {len(subjects)} ({', '.join(subjects)})")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„
        if 'step1' in self.validation_results and 'sequence_length_stats' in self.validation_results['step1']:
            stats = self.validation_results['step1']['sequence_length_stats']
            print(f"\nğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„:")
            print(f"  ë²”ìœ„: {stats['min_length']} ~ {stats['max_length']} (í‰ê· : {stats['mean_length']:.1f})")
            print(f"  P99: {stats['p99_length']:.1f}, Outlier ì„ê³„ê°’: {stats['outlier_threshold']:.1f}")
        
        # í–¥ìƒëœ ì™„ë£Œ ê¸°ì¤€ ì²´í¬
        print(f"\nâœ… í–¥ìƒëœ ì™„ë£Œ ê¸°ì¤€ ë‹¬ì„±:")
        print(f"  1. ì‹œí€€ìŠ¤ ê¸¸ì´ outlier ë¶„ì„ ë° í•„í„°ë§: âœ…")
        print(f"  2. ë³´ì¡° íŠ¹ì§• í†µê³„ ì €ì¥: âœ…")
        print(f"  3. LOSO êµ¬ì¡° ëª…í™•í™” ë° 2ì¤‘ ê²€ì¦: âœ…")
        print(f"  4. Foot ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦: âœ…")
        print(f"  5. ì¶œë ¥ signature ëª…ì‹œ: âœ…")
        print(f"  6. dtype ë° ì •ê·œí™” ì ìš© í™•ì¸: âœ…")
        
        print("\nğŸ¯ ë°ì´í„° í˜•íƒœ í™•ì¸:")
        print("  ì…ë ¥ 1: RaggedTensor (batch, None, 6) - ê°€ë³€ ê¸¸ì´ IMU ì‹œí€€ìŠ¤")
        print("  ì…ë ¥ 2: Dense Tensor (batch, 3) - [stride_time, height, foot_id]")
        print("  ì¶œë ¥: Dense Tensor (batch,) - stride_length")
        print("  ì •ê·œí™”: ì‹œí€€ìŠ¤(z-score), stride_time/height(z-score), foot_id(0/1/-1)")
        
        print("ğŸ‰" + "="*78 + "ğŸ‰")


if __name__ == "__main__":
    # í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = EnhancedStrideCVPipeline()
    results = pipeline.run_enhanced_pipeline()
    
    if results['status'] == 'SUCCESS':
        print("\nğŸ‰ ëª¨ë“  í–¥ìƒëœ ê²€ì¦ ì™„ë£Œ! êµì°¨ê²€ì¦ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâŒ í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")
        sys.exit(1)