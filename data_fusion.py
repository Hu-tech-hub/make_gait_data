#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IMU ë°ì´í„°ì™€ ë³´í–‰ ë¶„ì„ ê²°ê³¼ ìœµí•© ì‹œìŠ¤í…œ

make_data.pyì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ gait_analyzer.pyì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬
í†µí•©ëœ ë©€í‹°ëª¨ë‹¬ ë³´í–‰ ë¶„ì„ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. experiment_data ë””ë ‰í† ë¦¬ì—ì„œ ì„¸ì…˜ ë°ì´í„° ìë™ ìŠ¤ìº”
2. ê° ì„¸ì…˜ì˜ ë¹„ë””ì˜¤ì— ëŒ€í•´ ë³´í–‰ ë¶„ì„ ìˆ˜í–‰
3. IMU ë°ì´í„°ì™€ ë³´í–‰ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„/í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë™ê¸°í™”
4. í†µí•© ë¶„ì„ ê²°ê³¼ CSV ìƒì„±

ì‚¬ìš©ë²•:
    python data_fusion.py [--input-dir experiment_data] [--output-dir fusion_results]
"""

import os
import sys
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional

# ë¡œì»¬ ëª¨ë“ˆ import
from gait_class import GaitAnalyzer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_fusion.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class DataFusionProcessor:
    """IMUì™€ ë³´í–‰ ë¶„ì„ ë°ì´í„° ìœµí•© ì²˜ë¦¬ê¸°"""
    
    def __init__(self, input_dir: str = "experiment_data", output_dir: str = "fusion_results"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.fusion_results = []
        
    def scan_sessions(self) -> List[Path]:
        """experiment_data ë””ë ‰í† ë¦¬ì—ì„œ ì„¸ì…˜ í´ë”ë“¤ì„ ìŠ¤ìº”"""
        if not self.input_dir.exists():
            logger.error(f"ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.input_dir}")
            return []
        
        session_dirs = []
        for item in self.input_dir.iterdir():
            if item.is_dir() and item.name.startswith('session_'):
                # í•„ìˆ˜ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                video_file = item / "video.mp4"
                imu_file = item / "imu_data.csv"
                metadata_file = item / "metadata.json"
                
                if all([video_file.exists(), imu_file.exists(), metadata_file.exists()]):
                    session_dirs.append(item)
                    logger.info(f"ìœ íš¨í•œ ì„¸ì…˜ ë°œê²¬: {item.name}")
                else:
                    missing_files = []
                    if not video_file.exists(): missing_files.append("video.mp4")
                    if not imu_file.exists(): missing_files.append("imu_data.csv")
                    if not metadata_file.exists(): missing_files.append("metadata.json")
                    logger.warning(f"ì„¸ì…˜ {item.name}ì—ì„œ ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)}")
        
        session_dirs.sort()  # ì‹œê°„ìˆœ ì •ë ¬
        logger.info(f"ì´ {len(session_dirs)}ê°œì˜ ìœ íš¨í•œ ì„¸ì…˜ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return session_dirs
    
    def load_session_data(self, session_dir: Path) -> Tuple[Optional[pd.DataFrame], Optional[Dict]]:
        """ì„¸ì…˜ ë°ì´í„° ë¡œë“œ (IMU + ë©”íƒ€ë°ì´í„°)"""
        try:
            # IMU ë°ì´í„° ë¡œë“œ
            imu_file = session_dir / "imu_data.csv"
            imu_data = pd.read_csv(imu_file)
            logger.info(f"IMU ë°ì´í„° ë¡œë“œ: {len(imu_data)} ìƒ˜í”Œ")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_file = session_dir / "metadata.json"
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            return imu_data, metadata
            
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({session_dir.name}): {e}")
            return None, None
    
    def analyze_gait(self, session_dir: Path) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:
        """ë¹„ë””ì˜¤ì— ëŒ€í•´ ë³´í–‰ ë¶„ì„ ìˆ˜í–‰"""
        try:
            video_file = session_dir / "video.mp4"
            
            # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_output = self.output_dir / f"{session_dir.name}_gait_temp"
            temp_output.mkdir(exist_ok=True)
            
            # GaitAnalyzerë¡œ ë¶„ì„ ìˆ˜í–‰ (ê³ ì†ëª¨ë“œ)
            logger.info(f"ë³´í–‰ ë¶„ì„ ì‹œì‘: {session_dir.name}")
            analyzer = GaitAnalyzer(str(video_file), str(temp_output), enable_fast_mode=True)
            
            # 4ë‹¨ê³„ ë¶„ì„ ìˆ˜í–‰
            frame_mapping = analyzer.step1_prepare_video_data()
            joint_data = analyzer.step2_extract_joint_signals()
            events_data = analyzer.step3_detect_gait_events()
            analyzer.step4_visualize_and_export()
            
            logger.info(f"ë³´í–‰ ë¶„ì„ ì™„ë£Œ: ê´€ì ˆ ë°ì´í„° {joint_data.shape}, ì´ë²¤íŠ¸ {len(events_data)}")
            
            return joint_data, events_data
            
        except Exception as e:
            logger.error(f"ë³´í–‰ ë¶„ì„ ì‹¤íŒ¨ ({session_dir.name}): {e}")
            return None, None
    
    def synchronize_data(self, imu_data: pd.DataFrame, gait_data: pd.DataFrame, 
                        events_data: pd.DataFrame, metadata: Dict) -> pd.DataFrame:
        """IMU ë°ì´í„°ì™€ ë³´í–‰ ë¶„ì„ ê²°ê³¼ë¥¼ ë™ê¸°í™”"""
        logger.info("ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")
        
        # ê¸°ë³¸ í”„ë ˆì„ ì •ë³´ë¡œ ì‹œì‘ (gait_data ê¸°ì¤€)
        synchronized_data = gait_data.copy()
        
        # IMU ë°ì´í„° ë§¤í•‘ (frame_number ê¸°ì¤€)
        imu_dict = {}
        for _, row in imu_data.iterrows():
            frame_num = int(row['frame_number'])
            imu_dict[frame_num] = {
                'imu_accel_x': row['accel_x'],
                'imu_accel_y': row['accel_y'], 
                'imu_accel_z': row['accel_z'],
                'imu_gyro_x': row['gyro_x'],
                'imu_gyro_y': row['gyro_y'],
                'imu_gyro_z': row['gyro_z'],
                'imu_sync_timestamp': row['sync_timestamp']
            }
        
        # IMU ë°ì´í„°ë¥¼ gait_dataì— ì¶”ê°€
        imu_columns = ['imu_accel_x', 'imu_accel_y', 'imu_accel_z', 
                      'imu_gyro_x', 'imu_gyro_y', 'imu_gyro_z', 'imu_sync_timestamp']
        
        for col in imu_columns:
            synchronized_data[col] = np.nan
        
        for idx, row in synchronized_data.iterrows():
            frame_idx = int(row['frame_idx'])
            if frame_idx in imu_dict:
                for col in imu_columns:
                    synchronized_data.loc[idx, col] = imu_dict[frame_idx][col]
        
        # ë³´í–‰ ì´ë²¤íŠ¸ ì •ë³´ ì¶”ê°€
        synchronized_data['gait_event'] = ''
        synchronized_data['gait_event_details'] = ''
        
        for _, event in events_data.iterrows():
            event_frame = int(event['frame_idx'])
            event_type = event['event_type']
            
            # í•´ë‹¹ í”„ë ˆì„ì— ì´ë²¤íŠ¸ ì •ë³´ ì¶”ê°€
            mask = synchronized_data['frame_idx'] == event_frame
            if mask.any():
                current_events = synchronized_data.loc[mask, 'gait_event'].iloc[0]
                if current_events:
                    synchronized_data.loc[mask, 'gait_event'] = current_events + ',' + event_type
                else:
                    synchronized_data.loc[mask, 'gait_event'] = event_type
                
                # ìƒì„¸ ì •ë³´ ì¶”ê°€
                details = f"{event_type}(ankle_x:{event['ankle_x']:.3f},ankle_y:{event['ankle_y']:.3f})"
                current_details = synchronized_data.loc[mask, 'gait_event_details'].iloc[0]
                if current_details:
                    synchronized_data.loc[mask, 'gait_event_details'] = current_details + ';' + details
                else:
                    synchronized_data.loc[mask, 'gait_event_details'] = details
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
        synchronized_data['session_id'] = metadata['session_id']
        synchronized_data['session_duration'] = metadata['duration']
        synchronized_data['video_fps'] = metadata['video_fps']
        synchronized_data['imu_hz'] = metadata['imu_hz']
        
        # IMU ë°ì´í„°ê°€ ìˆëŠ” í–‰ë§Œ í•„í„°ë§ (ì˜µì…˜)
        valid_imu_mask = synchronized_data['imu_sync_timestamp'].notna()
        valid_data_count = valid_imu_mask.sum()
        total_data_count = len(synchronized_data)
        
        logger.info(f"ë™ê¸°í™” ì™„ë£Œ: ì „ì²´ {total_data_count} í”„ë ˆì„ ì¤‘ {valid_data_count} í”„ë ˆì„ì— IMU ë°ì´í„° ë§¤í•‘")
        
        return synchronized_data
    
    def calculate_multimodal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ê³„ì‚°"""
        logger.info("ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ê³„ì‚° ì¤‘...")
        
        # IMU ê¸°ë°˜ íŠ¹ì„±
        if 'imu_accel_x' in data.columns:
            # ê°€ì†ë„ í¬ê¸° (ë²¡í„° í¬ê¸°)
            data['imu_accel_magnitude'] = np.sqrt(
                data['imu_accel_x']**2 + data['imu_accel_y']**2 + data['imu_accel_z']**2
            )
            
            # ìì´ë¡œ í¬ê¸°
            data['imu_gyro_magnitude'] = np.sqrt(
                data['imu_gyro_x']**2 + data['imu_gyro_y']**2 + data['imu_gyro_z']**2
            )
            
            # ë³´í–‰ ë°©í–¥ ì¶”ì • (ì£¼ ì›€ì§ì„ ì¶•)
            data['imu_walking_axis'] = np.argmax(
                np.abs([data['imu_accel_x'], data['imu_accel_y'], data['imu_accel_z']]), axis=0
            )
        
        # ë¹„ì „ ê¸°ë°˜ íŠ¹ì„±ê³¼ IMU ìƒê´€ê´€ê³„
        if all(col in data.columns for col in ['left_ankle_x', 'imu_accel_x']):
            # ë°œëª© ì›€ì§ì„ê³¼ IMU ê°€ì†ë„ ìƒê´€ê´€ê³„ (ìœˆë„ìš°ë³„)
            window_size = 30  # 1ì´ˆ ìœˆë„ìš° (30fps)
            correlations = []
            
            for i in range(len(data)):
                start_idx = max(0, i - window_size//2)
                end_idx = min(len(data), i + window_size//2)
                
                if end_idx - start_idx > 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
                    window_data = data.iloc[start_idx:end_idx]
                    
                    # ë°œëª© x ì›€ì§ì„ê³¼ IMU x ê°€ì†ë„ ìƒê´€ê´€ê³„
                    ankle_x = window_data['left_ankle_x'].dropna()
                    imu_x = window_data['imu_accel_x'].dropna()
                    
                    if len(ankle_x) > 5 and len(imu_x) > 5:
                        # ê¸¸ì´ë¥¼ ë§ì¶°ì„œ ìƒê´€ê´€ê³„ ê³„ì‚°
                        min_len = min(len(ankle_x), len(imu_x))
                        if min_len > 5:
                            corr = np.corrcoef(ankle_x.iloc[:min_len], imu_x.iloc[:min_len])[0, 1]
                            if not np.isnan(corr):
                                correlations.append(corr)
                            else:
                                correlations.append(0.0)
                        else:
                            correlations.append(0.0)
                    else:
                        correlations.append(0.0)
                else:
                    correlations.append(0.0)
            
            data['vision_imu_correlation'] = correlations
        
        # ì´ë²¤íŠ¸ ê¸°ë°˜ íŠ¹ì„±
        data['is_gait_event'] = data['gait_event'] != ''
        data['hs_event_count'] = data['gait_event'].str.contains('HS', na=False).astype(int)
        data['to_event_count'] = data['gait_event'].str.contains('TO', na=False).astype(int)
        
        logger.info("ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ê³„ì‚° ì™„ë£Œ")
        return data
    
    def process_session(self, session_dir: Path) -> Optional[pd.DataFrame]:
        """ë‹¨ì¼ ì„¸ì…˜ ì „ì²´ ì²˜ë¦¬"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ì„¸ì…˜ ì²˜ë¦¬ ì‹œì‘: {session_dir.name}")
        logger.info(f"{'='*60}")
        
        # 1. ì„¸ì…˜ ë°ì´í„° ë¡œë“œ
        imu_data, metadata = self.load_session_data(session_dir)
        if imu_data is None or metadata is None:
            logger.error(f"ì„¸ì…˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {session_dir.name}")
            return None
        
        # 2. ë³´í–‰ ë¶„ì„ ìˆ˜í–‰
        gait_results = self.analyze_gait(session_dir)
        if gait_results is None:
            logger.error(f"ë³´í–‰ ë¶„ì„ ì‹¤íŒ¨: {session_dir.name}")
            return None
        
        joint_data, events_data = gait_results
        
        # 3. ë°ì´í„° ë™ê¸°í™”
        synchronized_data = self.synchronize_data(imu_data, joint_data, events_data, metadata)
        
        # 4. ë©€í‹°ëª¨ë‹¬ íŠ¹ì„± ê³„ì‚°
        enriched_data = self.calculate_multimodal_features(synchronized_data)
        
        # 5. ì„¸ì…˜ë³„ ê²°ê³¼ ì €ì¥
        session_output_file = self.output_dir / f"{session_dir.name}_fusion_result.csv"
        enriched_data.to_csv(session_output_file, index=False)
        logger.info(f"ì„¸ì…˜ ê²°ê³¼ ì €ì¥: {session_output_file}")
        
        # 6. ìš”ì•½ í†µê³„ ìƒì„±
        self.generate_session_summary(enriched_data, session_dir.name, metadata)
        
        return enriched_data
    
    def generate_session_summary(self, data: pd.DataFrame, session_name: str, metadata: Dict):
        """ì„¸ì…˜ë³„ ìš”ì•½ í†µê³„ ìƒì„±"""
        summary = {
            'session_name': session_name,
            'session_id': metadata['session_id'],
            'duration_seconds': metadata['duration'],
            'total_frames': len(data),
            'valid_imu_frames': data['imu_sync_timestamp'].notna().sum(),
            'total_gait_events': data['is_gait_event'].sum(),
            'hs_events': data['hs_event_count'].sum(),
            'to_events': data['to_event_count'].sum(),
            'avg_ankle_distance': data['ankle_distance'].mean() if 'ankle_distance' in data.columns else None,
            'avg_imu_accel_magnitude': data['imu_accel_magnitude'].mean() if 'imu_accel_magnitude' in data.columns else None,
            'avg_vision_imu_correlation': data['vision_imu_correlation'].mean() if 'vision_imu_correlation' in data.columns else None,
            'data_completeness': (data['imu_sync_timestamp'].notna().sum() / len(data) * 100) if len(data) > 0 else 0
        }
        
        self.fusion_results.append(summary)
        logger.info(f"ì„¸ì…˜ ìš”ì•½: {summary['total_gait_events']}ê°œ ì´ë²¤íŠ¸, "
                   f"{summary['data_completeness']:.1f}% ë°ì´í„° ì™„ì„±ë„")
    
    def process_all_sessions(self):
        """ëª¨ë“  ì„¸ì…˜ ì²˜ë¦¬"""
        logger.info(f"\nğŸš€ ë°ì´í„° ìœµí•© ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"ì…ë ¥ ë””ë ‰í† ë¦¬: {self.input_dir}")
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        
        session_dirs = self.scan_sessions()
        if not session_dirs:
            logger.error("ì²˜ë¦¬í•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_sessions = len(session_dirs)
        successful_sessions = 0
        
        all_fusion_data = []
        
        for i, session_dir in enumerate(session_dirs, 1):
            logger.info(f"\nğŸ“Š ì§„í–‰ë¥ : {i}/{total_sessions} ({i/total_sessions*100:.1f}%)")
            
            try:
                fusion_data = self.process_session(session_dir)
                if fusion_data is not None:
                    all_fusion_data.append(fusion_data)
                    successful_sessions += 1
                    logger.info(f"âœ… {session_dir.name} ì²˜ë¦¬ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ {session_dir.name} ì²˜ë¦¬ ì‹¤íŒ¨")
            except Exception as e:
                logger.error(f"âŒ {session_dir.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„±
        if all_fusion_data:
            self.generate_integrated_results(all_fusion_data)
        
        # ìµœì¢… ìš”ì•½
        logger.info(f"\nğŸ“ˆ ë°ì´í„° ìœµí•© ì™„ë£Œ")
        logger.info(f"ì´ ì„¸ì…˜: {total_sessions}")
        logger.info(f"ì„±ê³µí•œ ì„¸ì…˜: {successful_sessions}")
        logger.info(f"ì‹¤íŒ¨í•œ ì„¸ì…˜: {total_sessions - successful_sessions}")
        logger.info(f"ì„±ê³µë¥ : {successful_sessions/total_sessions*100:.1f}%")
    
    def generate_integrated_results(self, all_data: List[pd.DataFrame]):
        """í†µí•© ê²°ê³¼ ìƒì„±"""
        logger.info("í†µí•© ê²°ê³¼ ìƒì„± ì¤‘...")
        
        # 1. ì „ì²´ ë°ì´í„° í†µí•©
        integrated_data = pd.concat(all_data, ignore_index=True)
        integrated_file = self.output_dir / "integrated_fusion_results.csv"
        integrated_data.to_csv(integrated_file, index=False)
        logger.info(f"í†µí•© ë°ì´í„° ì €ì¥: {integrated_file} ({len(integrated_data)} í–‰)")
        
        # 2. ì„¸ì…˜ë³„ ìš”ì•½ ì €ì¥
        if self.fusion_results:
            summary_df = pd.DataFrame(self.fusion_results)
            summary_file = self.output_dir / "session_summaries.csv"
            summary_df.to_csv(summary_file, index=False)
            logger.info(f"ì„¸ì…˜ ìš”ì•½ ì €ì¥: {summary_file}")
        
        # 3. ì „ì²´ í†µê³„ ìƒì„±
        overall_stats = {
            'total_sessions': len(self.fusion_results),
            'total_frames': len(integrated_data),
            'total_gait_events': integrated_data['is_gait_event'].sum(),
            'total_hs_events': integrated_data['hs_event_count'].sum(),
            'total_to_events': integrated_data['to_event_count'].sum(),
            'avg_data_completeness': np.mean([r['data_completeness'] for r in self.fusion_results]),
            'avg_session_duration': np.mean([r['duration_seconds'] for r in self.fusion_results]),
            'processing_timestamp': datetime.now().isoformat()
        }
        
        stats_file = self.output_dir / "overall_statistics.json"
        with open(stats_file, 'w') as f:
            json.dump(overall_stats, f, indent=2)
        
        logger.info(f"ì „ì²´ í†µê³„ ì €ì¥: {stats_file}")
        logger.info(f"ğŸ“Š ì „ì²´ í†µê³„: {overall_stats['total_sessions']}ê°œ ì„¸ì…˜, "
                   f"{overall_stats['total_gait_events']}ê°œ ì´ë²¤íŠ¸, "
                   f"{overall_stats['avg_data_completeness']:.1f}% í‰ê·  ì™„ì„±ë„")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='IMU-ë³´í–‰ë¶„ì„ ë°ì´í„° ìœµí•© ì‹œìŠ¤í…œ')
    parser.add_argument('--input-dir', type=str, default='experiment_data',
                       help='ì…ë ¥ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: experiment_data)')
    parser.add_argument('--output-dir', type=str, default='fusion_results',
                       help='ì¶œë ¥ ê²°ê³¼ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: fusion_results)')
    parser.add_argument('--session', type=str, default=None,
                       help='íŠ¹ì • ì„¸ì…˜ë§Œ ì²˜ë¦¬ (ì˜ˆ: session_20250604_210219)')
    
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info("ğŸ”— IMU-ë³´í–‰ë¶„ì„ ë°ì´í„° ìœµí•© ì‹œìŠ¤í…œ")
    logger.info("=" * 80)
    
    # ë°ì´í„° ìœµí•© í”„ë¡œì„¸ì„œ ìƒì„±
    processor = DataFusionProcessor(args.input_dir, args.output_dir)
    
    if args.session:
        # íŠ¹ì • ì„¸ì…˜ë§Œ ì²˜ë¦¬
        session_path = Path(args.input_dir) / args.session
        if session_path.exists():
            logger.info(f"íŠ¹ì • ì„¸ì…˜ ì²˜ë¦¬: {args.session}")
            processor.process_session(session_path)
        else:
            logger.error(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_path}")
    else:
        # ëª¨ë“  ì„¸ì…˜ ì²˜ë¦¬
        processor.process_all_sessions()


if __name__ == "__main__":
    main() 