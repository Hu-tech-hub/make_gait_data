#!/usr/bin/env python3
"""
TCN ê¸°ë°˜ ë³´í­ ì˜ˆì¸¡ ëª¨ë¸ êµì°¨ê²€ì¦ íŠ¸ë ˆì´ë„ˆ

í•™ìŠµ íë¦„:
1. metadata/file_index.csvì™€ metadata/cv_splits.json ë¡œë“œ
2. ë¬´ê²°ì„± ê²€ì‚¬ (PKL ê²½ë¡œ, ì¤‘ë³µ, í”¼í—˜ì ëˆ„ìˆ˜)
3. RaggedStrideDataGeneratorë¡œ foldë³„ ë°ì´í„°ì…‹ ìƒì„±
4. TCN ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ
5. EarlyStopping/ReduceLROnPlateau ì½œë°± ì ìš©
6. ìµœì  ëª¨ë¸ ì €ì¥ (models/best_fold{k}.keras)
7. êµì°¨ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± (models/cv_report.txt)
8. ì„ íƒì  ì „ì²´ ì¬í›ˆë ¨ (models/final_model.keras)
"""

import os
import sys
import json
import argparse
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import tensorflow as tf

# ë¡œì»¬ ëª¨ë“ˆ import
from tcn_model import create_tcn_model, get_model_callbacks
from ragged_data_generator import RaggedStrideDataGenerator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('tcn_training.log')
    ]
)
logger = logging.getLogger(__name__)

class TCNTrainer:
    """TCN ëª¨ë¸ êµì°¨ê²€ì¦ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, 
                 metadata_dir: str = "metadata",
                 pkl_dir: str = "stride_train_data_pkl",
                 models_dir: str = "models",
                 logs_dir: str = "logs",
                 strict_mode: bool = True):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            metadata_dir: ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬
            pkl_dir: PKL íŒŒì¼ ë””ë ‰í† ë¦¬
            models_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            logs_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            strict_mode: ì—„ê²© ëª¨ë“œ (ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨ì‹œ ì¤‘ë‹¨)
        """
        self.metadata_dir = Path(metadata_dir)
        self.pkl_dir = Path(pkl_dir)
        self.models_dir = Path(models_dir)
        self.logs_dir = Path(logs_dir)
        self.strict_mode = strict_mode
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.models_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ì œë„ˆë ˆì´í„°
        self.generator = None
        
        # ê²°ê³¼ ì €ì¥
        self.cv_results = []
        self.training_history = {}
        
    def load_and_validate_metadata(self) -> Tuple[pd.DataFrame, List[Dict]]:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        
        logger.info("=== ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ ===")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_index_path = self.metadata_dir / 'file_index.csv'
        cv_splits_path = self.metadata_dir / 'cv_splits.json'
        
        if not file_index_path.exists():
            raise FileNotFoundError(f"file_index.csv not found: {file_index_path}")
        
        if not cv_splits_path.exists():
            raise FileNotFoundError(f"cv_splits.json not found: {cv_splits_path}")
        
        # íŒŒì¼ ë¡œë“œ
        logger.info(f"ğŸ“ Loading file index: {file_index_path}")
        df = pd.read_csv(file_index_path)
        
        logger.info(f"ğŸ“ Loading CV splits: {cv_splits_path}")
        with open(cv_splits_path, 'r', encoding='utf-8') as f:
            cv_splits = json.load(f)
        
        logger.info(f"âœ… Loaded {len(df)} files, {len(cv_splits)} folds")
        
        # ë¬´ê²°ì„± ê²€ì‚¬
        self._validate_data_integrity(df, cv_splits)
        
        return df, cv_splits
    
    def _validate_data_integrity(self, df: pd.DataFrame, cv_splits: List[Dict]):
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬"""
        
        logger.info("ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘...")
        
        issues = []
        
        # 1. PKL íŒŒì¼ ì¡´ì¬ í™•ì¸
        missing_files = []
        for _, row in df.iterrows():
            pkl_path = self.pkl_dir / row['file_name']
            if not pkl_path.exists():
                missing_files.append(row['file_name'])
        
        if missing_files:
            issues.append(f"Missing PKL files: {len(missing_files)} files")
            if len(missing_files) <= 5:
                issues.append(f"  Examples: {missing_files}")
        
        # 2. CV splits íŒŒì¼ ì¤‘ë³µ í™•ì¸
        all_files_in_splits = set()
        duplicate_files = []
        
        for fold in cv_splits:
            train_files = set(fold['train_files'])
            val_files = set(fold['val_files'])
            
            # ê°™ì€ fold ë‚´ ì¤‘ë³µ
            overlap = train_files & val_files
            if overlap:
                issues.append(f"Fold {fold['fold']}: train-val overlap: {len(overlap)} files")
            
            # ì „ì²´ ì¤‘ë³µ
            for f in train_files | val_files:
                if f in all_files_in_splits:
                    duplicate_files.append(f)
                all_files_in_splits.add(f)
        
        if duplicate_files:
            issues.append(f"Duplicate files across folds: {len(set(duplicate_files))} files")
        
        # 3. í”¼í—˜ì ëˆ„ìˆ˜ í™•ì¸
        for fold in cv_splits:
            test_subject = fold['test_subject']
            train_subjects = set(fold['train_subjects'])
            
            if test_subject in train_subjects:
                issues.append(f"Fold {fold['fold']}: subject leak: {test_subject} in both train and test")
        
        # 4. íŒŒì¼ ì»¤ë²„ë¦¬ì§€ í™•ì¸
        df_files = set(df['file_name'].tolist())
        split_files = all_files_in_splits
        
        missing_in_splits = df_files - split_files
        extra_in_splits = split_files - df_files
        
        if missing_in_splits:
            issues.append(f"Files in index but not in splits: {len(missing_in_splits)}")
        
        if extra_in_splits:
            issues.append(f"Files in splits but not in index: {len(extra_in_splits)}")
        
        # ê²°ê³¼ ì²˜ë¦¬
        if issues:
            logger.warning(f"âŒ ë¬´ê²°ì„± ê²€ì‚¬ì—ì„œ {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬:")
            for issue in issues:
                logger.warning(f"  - {issue}")
            
            if self.strict_mode:
                raise ValueError(f"Strict mode: Data integrity check failed with {len(issues)} issues")
            else:
                logger.warning("âš ï¸ ë¹„ì—„ê²© ëª¨ë“œ: ì´ìŠˆê°€ ìˆì§€ë§Œ ê³„ì† ì§„í–‰")
        else:
            logger.info("âœ… ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
    
    def initialize_data_generator(self):
        """ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”"""
        
        logger.info("ğŸ“Š ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™” ì¤‘...")
        
        self.generator = RaggedStrideDataGenerator(
            pkl_dir=str(self.pkl_dir),
            metadata_dir=str(self.metadata_dir)
        )
        
        if self.generator.cv_splits is None:
            raise ValueError("Failed to load CV splits in data generator")
        
        if self.generator.normalization_stats is None:
            raise ValueError("Failed to load normalization stats in data generator")
        
        logger.info(f"âœ… ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   CV folds: {len(self.generator.cv_splits)}")
        logger.info(f"   Normalization stats: âœ“")
        logger.info(f"   Auxiliary stats: {'âœ“' if self.generator.auxiliary_stats else 'âœ—'}")
    
    def train_single_fold(self, 
                         fold_idx: int,
                         model_config: Dict,
                         training_config: Dict) -> Dict:
        """ë‹¨ì¼ fold í•™ìŠµ"""
        
        logger.info(f"\n{'='*60}")
        logger.info(f"FOLD {fold_idx + 1} í•™ìŠµ ì‹œì‘")
        logger.info(f"{'='*60}")
        
        fold_start_time = datetime.now()
        
        # ë°ì´í„°ì…‹ ìƒì„±
        logger.info("ğŸ“Š ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        train_ds, val_ds, fold_info = self.generator.get_fold_datasets(
            fold_idx=fold_idx,
            batch_size=training_config['batch_size'],
            normalize=True
        )
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        logger.info(f"   Test subject: {fold_info['test_subject']}")
        logger.info(f"   Train cycles: {fold_info['n_train_cycles']:,}")
        logger.info(f"   Val cycles: {fold_info['n_val_cycles']:,}")
        
        # ëª¨ë¸ ìƒì„±
        logger.info("ğŸ§  ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_tcn_model(**model_config)
        
        logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        logger.info(f"   ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        
        # ì½œë°± í•¨ìˆ˜ ì¤€ë¹„
        model_save_path = self.models_dir / f"best_fold_{fold_idx + 1}.keras"
        callbacks = get_model_callbacks(
            patience_early=training_config['patience_early'],
            patience_lr=training_config['patience_lr'],
            lr_factor=training_config['lr_factor'],
            min_lr=training_config['min_lr'],
            monitor='val_loss',
            save_best_path=str(model_save_path)
        )
        
        # í•™ìŠµ ì‹¤í–‰
        logger.info(f"ğŸƒ í•™ìŠµ ì‹œì‘ (ìµœëŒ€ {training_config['epochs']} epochs)...")
        
        history = model.fit(
            train_ds,
            validation_data=val_ds,
            epochs=training_config['epochs'],
            callbacks=callbacks,
            verbose=1
        )
        
        # í•™ìŠµ ê²°ê³¼ ë¶„ì„
        fold_duration = (datetime.now() - fold_start_time).total_seconds() / 60
        
        best_epoch = np.argmin(history.history['val_loss']) + 1
        best_val_loss = np.min(history.history['val_loss'])
        best_val_mae = history.history['val_mae'][best_epoch - 1]
        
        final_train_loss = history.history['loss'][-1]
        final_train_mae = history.history['mae'][-1]
        
        fold_result = {
            'fold': fold_idx + 1,
            'test_subject': fold_info['test_subject'],
            'train_subjects': fold_info['train_subjects'],
            'n_train_cycles': fold_info['n_train_cycles'],
            'n_val_cycles': fold_info['n_val_cycles'],
            'best_epoch': best_epoch,
            'total_epochs': len(history.history['loss']),
            'best_val_loss': best_val_loss,
            'best_val_mae': best_val_mae,
            'final_train_loss': final_train_loss,
            'final_train_mae': final_train_mae,
            'duration_minutes': fold_duration,
            'model_path': str(model_save_path)
        }
        
        logger.info(f"âœ… Fold {fold_idx + 1} í•™ìŠµ ì™„ë£Œ")
        logger.info(f"   ìµœì  epoch: {best_epoch}/{len(history.history['loss'])}")
        logger.info(f"   ìµœì  val MAE: {best_val_mae:.4f}")
        logger.info(f"   ìµœì¢… train MAE: {final_train_mae:.4f}")
        logger.info(f"   í•™ìŠµ ì‹œê°„: {fold_duration:.1f}ë¶„")
        logger.info(f"   ëª¨ë¸ ì €ì¥: {model_save_path}")
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        history_path = self.logs_dir / f"fold_{fold_idx + 1}_history.json"
        history_dict = {k: [float(v) for v in values] for k, values in history.history.items()}
        with open(history_path, 'w') as f:
            json.dump(history_dict, f, indent=2)
        
        return fold_result
    
    def run_cross_validation(self,
                           model_config: Optional[Dict] = None,
                           training_config: Optional[Dict] = None) -> Dict:
        """êµì°¨ê²€ì¦ ì‹¤í–‰"""
        
        logger.info("\n" + "ğŸš€" + "="*58 + "ğŸš€")
        logger.info("TCN êµì°¨ê²€ì¦ í•™ìŠµ ì‹œì‘")
        logger.info("ğŸš€" + "="*58 + "ğŸš€")
        
        cv_start_time = datetime.now()
        
        # ê¸°ë³¸ ì„¤ì •
        if model_config is None:
            model_config = {
                'tcn_filters': 64,
                'tcn_stacks': 4,
                'dropout_rate': 0.1,
                'dense_units': 64,
                'learning_rate': 1e-3
            }
        
        if training_config is None:
            training_config = {
                'epochs': 100,
                'batch_size': 32,
                'patience_early': 10,
                'patience_lr': 5,
                'lr_factor': 0.5,
                'min_lr': 1e-6
            }
        
        logger.info(f"ğŸ“‹ ëª¨ë¸ ì„¤ì •: {model_config}")
        logger.info(f"ğŸ“‹ í•™ìŠµ ì„¤ì •: {training_config}")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        df, cv_splits = self.load_and_validate_metadata()
        
        # ë°ì´í„° ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”
        self.initialize_data_generator()
        
        # ê° fold í•™ìŠµ
        self.cv_results = []
        n_folds = len(cv_splits)
        
        for fold_idx in range(n_folds):
            try:
                fold_result = self.train_single_fold(
                    fold_idx=fold_idx,
                    model_config=model_config,
                    training_config=training_config
                )
                self.cv_results.append(fold_result)
                
            except Exception as e:
                logger.error(f"âŒ Fold {fold_idx + 1} í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                if self.strict_mode:
                    raise
                else:
                    logger.warning(f"âš ï¸ ë¹„ì—„ê²© ëª¨ë“œ: Fold {fold_idx + 1} ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰")
                    continue
        
        # êµì°¨ê²€ì¦ ê²°ê³¼ ë¶„ì„
        cv_duration = (datetime.now() - cv_start_time).total_seconds() / 60
        cv_summary = self._analyze_cv_results(cv_duration)
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_cv_report(cv_summary, model_config, training_config)
        
        logger.info(f"\nğŸ‰ êµì°¨ê²€ì¦ ì™„ë£Œ!")
        logger.info(f"   ì™„ë£Œëœ folds: {len(self.cv_results)}/{n_folds}")
        logger.info(f"   í‰ê·  val MAE: {cv_summary['mean_val_mae']:.4f} Â± {cv_summary['std_val_mae']:.4f}")
        logger.info(f"   ì´ ì†Œìš” ì‹œê°„: {cv_duration:.1f}ë¶„")
        
        return cv_summary
    
    def _analyze_cv_results(self, cv_duration: float) -> Dict:
        """êµì°¨ê²€ì¦ ê²°ê³¼ ë¶„ì„"""
        
        if not self.cv_results:
            raise ValueError("No fold results to analyze")
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        val_maes = [r['best_val_mae'] for r in self.cv_results]
        val_losses = [r['best_val_loss'] for r in self.cv_results]
        train_maes = [r['final_train_mae'] for r in self.cv_results]
        
        # í†µê³„ ê³„ì‚°
        summary = {
            'n_completed_folds': len(self.cv_results),
            'mean_val_mae': np.mean(val_maes),
            'std_val_mae': np.std(val_maes),
            'min_val_mae': np.min(val_maes),
            'max_val_mae': np.max(val_maes),
            'mean_val_loss': np.mean(val_losses),
            'std_val_loss': np.std(val_losses),
            'mean_train_mae': np.mean(train_maes),
            'std_train_mae': np.std(train_maes),
            'total_duration_minutes': cv_duration,
            'avg_duration_per_fold': cv_duration / len(self.cv_results),
            'fold_results': self.cv_results
        }
        
        return summary
    
    def _generate_cv_report(self, cv_summary: Dict, model_config: Dict, training_config: Dict):
        """êµì°¨ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report_path = self.models_dir / 'cv_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("TCN êµì°¨ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
            f.write("="*80 + "\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
            f.write("ğŸ“‹ ëª¨ë¸ ì„¤ì •:\n")
            for key, value in model_config.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            f.write("ğŸ“‹ í•™ìŠµ ì„¤ì •:\n")
            for key, value in training_config.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            f.write("ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½:\n")
            f.write(f"  ì™„ë£Œëœ folds: {cv_summary['n_completed_folds']}\n")
            f.write(f"  í‰ê·  val MAE: {cv_summary['mean_val_mae']:.4f} Â± {cv_summary['std_val_mae']:.4f}\n")
            f.write(f"  ìµœì†Œ val MAE: {cv_summary['min_val_mae']:.4f}\n")
            f.write(f"  ìµœëŒ€ val MAE: {cv_summary['max_val_mae']:.4f}\n")
            f.write(f"  í‰ê·  train MAE: {cv_summary['mean_train_mae']:.4f} Â± {cv_summary['std_train_mae']:.4f}\n")
            f.write(f"  ì´ ì†Œìš” ì‹œê°„: {cv_summary['total_duration_minutes']:.1f}ë¶„\n")
            f.write(f"  foldë‹¹ í‰ê·  ì‹œê°„: {cv_summary['avg_duration_per_fold']:.1f}ë¶„\n\n")
            
            # Foldë³„ ìƒì„¸ ê²°ê³¼
            f.write("ğŸ“ Foldë³„ ìƒì„¸ ê²°ê³¼:\n")
            f.write("-" * 120 + "\n")
            f.write(f"{'Fold':<4} {'Test Subject':<12} {'Train':<6} {'Val':<6} {'Best Epoch':<10} {'Val MAE':<8} {'Train MAE':<9} {'Duration':<8} {'Model Path':<30}\n")
            f.write("-" * 120 + "\n")
            
            for result in cv_summary['fold_results']:
                f.write(f"{result['fold']:<4} "
                       f"{result['test_subject']:<12} "
                       f"{result['n_train_cycles']:<6} "
                       f"{result['n_val_cycles']:<6} "
                       f"{result['best_epoch']:<10} "
                       f"{result['best_val_mae']:<8.4f} "
                       f"{result['final_train_mae']:<9.4f} "
                       f"{result['duration_minutes']:<8.1f} "
                       f"{Path(result['model_path']).name:<30}\n")
            
            f.write("-" * 120 + "\n")
        
        logger.info(f"ğŸ“„ êµì°¨ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    def retrain_final_model(self,
                           model_config: Optional[Dict] = None,
                           training_config: Optional[Dict] = None) -> str:
        """ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ ì¬í›ˆë ¨"""
        
        logger.info("\n" + "ğŸ”„" + "="*58 + "ğŸ”„")
        logger.info("ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ ì¬í›ˆë ¨")
        logger.info("ğŸ”„" + "="*58 + "ğŸ”„")
        
        if model_config is None:
            model_config = {
                'tcn_filters': 64,
                'tcn_stacks': 4,
                'dropout_rate': 0.1,
                'dense_units': 64,
                'learning_rate': 1e-3
            }
        
        if training_config is None:
            training_config = {
                'epochs': 50,  # ì¬í›ˆë ¨ì€ ë” ì ì€ epoch
                'batch_size': 32,
                'patience_early': 10,
                'patience_lr': 5,
                'lr_factor': 0.5,
                'min_lr': 1e-6
            }
        
        # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± (train + val í•©ì¹¨)
        logger.info("ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        # ëª¨ë“  foldì˜ train + val ë°ì´í„°ë¥¼ í•©ì³ì„œ ì‚¬ìš©
        all_train_data = []
        all_val_data = []
        
        for fold_idx in range(len(self.generator.cv_splits)):
            train_ds, val_ds, _ = self.generator.get_fold_datasets(
                fold_idx=fold_idx,
                batch_size=training_config['batch_size'],
                normalize=True
            )
            all_train_data.append(train_ds)
            all_val_data.append(val_ds)
        
        # ë°ì´í„°ì…‹ í•©ì¹˜ê¸° (ì²« ë²ˆì§¸ foldë¥¼ ì „ì²´ ë°ì´í„°ë¡œ ì‚¬ìš©)
        full_train_ds, _, fold_info = self.generator.get_fold_datasets(
            fold_idx=0,
            batch_size=training_config['batch_size'],
            normalize=True
        )
        
        total_cycles = sum(len(self.generator.cv_splits[i]['train_files']) + 
                          len(self.generator.cv_splits[i]['val_files']) 
                          for i in range(len(self.generator.cv_splits)))
        
        logger.info(f"âœ… ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
        logger.info(f"   ì´ cycles: ì•½ {total_cycles:,}")
        
        # ëª¨ë¸ ìƒì„±
        logger.info("ğŸ§  ìµœì¢… ëª¨ë¸ ìƒì„± ì¤‘...")
        final_model = create_tcn_model(**model_config)
        
        # ì½œë°± í•¨ìˆ˜ (validation ì—†ì´)
        final_model_path = self.models_dir / "final_model.keras"
        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                filepath=str(final_model_path),
                save_best_only=False,
                verbose=1
            )
        ]
        
        # ì¬í›ˆë ¨ ì‹¤í–‰
        logger.info(f"ğŸƒ ìµœì¢… ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘...")
        
        history = final_model.fit(
            full_train_ds,
            epochs=training_config['epochs'],
            callbacks=callbacks,
            verbose=1
        )
        
        logger.info(f"âœ… ìµœì¢… ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ")
        logger.info(f"   ìµœì¢… loss: {history.history['loss'][-1]:.4f}")
        logger.info(f"   ìµœì¢… MAE: {history.history['mae'][-1]:.4f}")
        logger.info(f"   ëª¨ë¸ ì €ì¥: {final_model_path}")
        
        return str(final_model_path)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    
    parser = argparse.ArgumentParser(description='TCN êµì°¨ê²€ì¦ íŠ¸ë ˆì´ë„ˆ')
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    parser.add_argument('--metadata_dir', default='metadata', help='ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--pkl_dir', default='stride_train_data_pkl', help='PKL íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--models_dir', default='models', help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--logs_dir', default='logs', help='ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--tcn_filters', type=int, default=64, help='TCN í•„í„° ìˆ˜')
    parser.add_argument('--tcn_stacks', type=int, default=4, help='TCN ìŠ¤íƒ ìˆ˜')
    parser.add_argument('--dropout_rate', type=float, default=0.1, help='ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨')
    parser.add_argument('--dense_units', type=int, default=64, help='Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=1e-3, help='í•™ìŠµë¥ ')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--epochs', type=int, default=100, help='ìµœëŒ€ epoch ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--patience_early', type=int, default=10, help='EarlyStopping patience')
    parser.add_argument('--patience_lr', type=int, default=5, help='ReduceLROnPlateau patience')
    parser.add_argument('--lr_factor', type=float, default=0.5, help='í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨')
    parser.add_argument('--min_lr', type=float, default=1e-6, help='ìµœì†Œ í•™ìŠµë¥ ')
    
    # ê¸°íƒ€ ì˜µì…˜
    parser.add_argument('--strict_mode', action='store_true', help='ì—„ê²© ëª¨ë“œ (ê¸°ë³¸ê°’: False)')
    parser.add_argument('--full_retrain', action='store_true', help='ì „ì²´ ì¬í›ˆë ¨ ìˆ˜í–‰')
    parser.add_argument('--export_tflite', action='store_true', help='TensorFlow Lite ë³€í™˜')
    
    return parser.parse_args()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    args = parse_arguments()
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = TCNTrainer(
        metadata_dir=args.metadata_dir,
        pkl_dir=args.pkl_dir,
        models_dir=args.models_dir,
        logs_dir=args.logs_dir,
        strict_mode=args.strict_mode
    )
    
    # ëª¨ë¸ ì„¤ì •
    model_config = {
        'tcn_filters': args.tcn_filters,
        'tcn_stacks': args.tcn_stacks,
        'dropout_rate': args.dropout_rate,
        'dense_units': args.dense_units,
        'learning_rate': args.learning_rate
    }
    
    # í•™ìŠµ ì„¤ì •
    training_config = {
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'patience_early': args.patience_early,
        'patience_lr': args.patience_lr,
        'lr_factor': args.lr_factor,
        'min_lr': args.min_lr
    }
    
    try:
        # êµì°¨ê²€ì¦ ì‹¤í–‰
        cv_summary = trainer.run_cross_validation(
            model_config=model_config,
            training_config=training_config
        )
        
        # ì „ì²´ ì¬í›ˆë ¨ (ì„ íƒì )
        if args.full_retrain:
            final_model_path = trainer.retrain_final_model(
                model_config=model_config,
                training_config=training_config
            )
            logger.info(f"ğŸ¯ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
        
        # TensorFlow Lite ë³€í™˜ (ì„ íƒì )
        if args.export_tflite:
            logger.info("ğŸ”„ TensorFlow Lite ë³€í™˜ ì¤‘...")
            # TODO: TFLite ë³€í™˜ êµ¬í˜„
            logger.info("âš ï¸ TensorFlow Lite ë³€í™˜ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        logger.info("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()