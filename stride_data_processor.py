#!/usr/bin/env python3
"""
Stride Training Data Processor
stride_train_data í´ë”ì˜ JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµìš© PKL ë°ì´í„°ë¡œ ë³€í™˜
Subject-wise LOSO 5-Fold êµì°¨ê²€ì¦ ì§€ì›
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
import re
from tqdm import tqdm
import logging
from sklearn.model_selection import LeaveOneGroupOut

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StrideDataProcessor:
    def __init__(self, input_dir="stride_train_data", output_dir="stride_train_data_pkl", metadata_dir="metadata"):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_dir: JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ì…ë ¥ ë””ë ‰í† ë¦¬
            output_dir: PKL íŒŒì¼ë“¤ì„ ì €ì¥í•  ì¶œë ¥ ë””ë ‰í† ë¦¬
            metadata_dir: ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ì¸ë±ìŠ¤, ì •ê·œí™”, CV splits ë“±)
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.metadata_dir = Path(metadata_dir)
        self.min_sequence_length = 15
        self.max_sequence_length = 100
        
        # í†µê³„ ì •ë³´ ì €ì¥
        self.processed_stats = {
            'total_sessions': 0,
            'valid_sessions': 0,
            'total_cycles': 0,
            'valid_cycles': 0,
            'filtered_cycles': 0,
            'empty_sessions': 0
        }
        
        # ì „ì—­ ì •ê·œí™”ë¥¼ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
        self.all_sequences = []
        
        # íŒŒì¼ ì¸ë±ìŠ¤ ë°ì´í„°
        self.file_index = []

    def parse_session_name(self, session_name):
        """
        ì„¸ì…˜ ì´ë¦„ì—ì„œ subject, task, rep ì¶”ì¶œ
        ì˜ˆ: S01T01R01 -> subject="S01", task="T01", rep="R01"
        Subject-wise LOSOë¥¼ ìœ„í•´ subjectëŠ” "S01" í˜•íƒœë¡œ ìœ ì§€
        """
        pattern = r'(S\d+)(T\d+)(R\d+)'
        match = re.match(pattern, session_name)
        if match:
            return match.groups()  # ("S01", "T01", "R01")
        else:
            logger.warning(f"Invalid session name format: {session_name}")
            return None, None, None

    def is_valid_sequence(self, sequence):
        """
        ì‹œí€€ìŠ¤ê°€ ìœ íš¨í•œì§€ ê²€ì‚¬
        - ê¸¸ì´ê°€ 15~100 ì‚¬ì´ì¸ì§€
        - NaN, Inf ê°’ì´ ì—†ëŠ”ì§€
        - ë¹ˆ ë°°ì—´ì´ ì•„ë‹Œì§€
        """
        if not sequence or len(sequence) == 0:
            return False
        
        if len(sequence) < self.min_sequence_length or len(sequence) > self.max_sequence_length:
            return False
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ NaN, Inf ì²´í¬
        try:
            seq_array = np.array(sequence)
            if np.any(np.isnan(seq_array)) or np.any(np.isinf(seq_array)):
                return False
        except (ValueError, TypeError):
            return False
        
        return True

    def filter_cycles(self, cycles):
        """
        ì‚¬ì´í´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ì´í´ë“¤ì„ í•„í„°ë§
        """
        valid_cycles = []
        filtered_count = 0
        
        for cycle in cycles:
            if 'sequence' not in cycle:
                filtered_count += 1
                continue
                
            if self.is_valid_sequence(cycle['sequence']):
                valid_cycles.append(cycle)
            else:
                filtered_count += 1
        
        return valid_cycles, filtered_count

    def process_session(self, session_path):
        """
        ê°œë³„ ì„¸ì…˜ í´ë” ì²˜ë¦¬
        """
        session_name = session_path.name
        subject, task, rep = self.parse_session_name(session_name)
        
        if not all([subject, task, rep]):
            logger.warning(f"Skipping invalid session: {session_name}")
            return None
        
        # JSON íŒŒì¼ ì°¾ê¸°
        json_file = session_path / f"{session_name}_Cycles.json"
        if not json_file.exists():
            logger.warning(f"JSON file not found: {json_file}")
            return None
        
        try:
            # JSON ë¡œë“œ
            with open(json_file, 'r', encoding='utf-8') as f:
                cycles_data = json.load(f)
            
            self.processed_stats['total_cycles'] += len(cycles_data)
            
            # ì‚¬ì´í´ í•„í„°ë§
            valid_cycles, filtered_count = self.filter_cycles(cycles_data)
            self.processed_stats['filtered_cycles'] += filtered_count
            
            if len(valid_cycles) == 0:
                logger.info(f"Empty session after filtering: {session_name}")
                self.processed_stats['empty_sessions'] += 1
                return None
            
            self.processed_stats['valid_cycles'] += len(valid_cycles)
            
            # ì „ì—­ ì •ê·œí™”ë¥¼ ìœ„í•´ ì‹œí€€ìŠ¤ ìˆ˜ì§‘
            for cycle in valid_cycles:
                self.all_sequences.extend(cycle['sequence'])
            
            # PKL ë°ì´í„° êµ¬ì¡° ìƒì„± (Subject-wise LOSOë¥¼ ìœ„í•œ êµ¬ì¡°)
            pkl_data = {
                'subject': subject,    # "S01" í˜•íƒœë¡œ ì €ì¥
                'task': task,         # "T01" í˜•íƒœë¡œ ì €ì¥
                'rep': rep,           # "R01" í˜•íƒœë¡œ ì €ì¥
                'cycles': valid_cycles
            }
            
            # PKL íŒŒì¼ ì €ì¥
            pkl_file = self.output_dir / f"{session_name}_Cycles.pkl"
            with open(pkl_file, 'wb') as f:
                pickle.dump(pkl_data, f)
            
            # íŒŒì¼ ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.file_index.append({
                'file_name': f"{session_name}_Cycles.pkl",
                'subject': subject,
                'task': task,
                'rep': rep,
                'n_cycles': len(valid_cycles)
            })
            
            logger.info(f"Processed {session_name}: {len(valid_cycles)} valid cycles")
            return pkl_data
            
        except Exception as e:
            logger.error(f"Error processing {session_name}: {str(e)}")
            return None

    def calculate_global_normalization(self):
        """
        ì „ì—­ ì •ê·œí™” í†µê³„ ê³„ì‚° (6ì¶• ê°ê°ì˜ í‰ê· , í‘œì¤€í¸ì°¨)
        """
        if not self.all_sequences:
            logger.warning("No sequences collected for normalization")
            return None
        
        # ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
        all_data = np.array(self.all_sequences)  # shape: (N, 6)
        
        # 6ì¶• ê°ê°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        global_mean = np.mean(all_data, axis=0)  # shape: (6,)
        global_std = np.std(all_data, axis=0)    # shape: (6,)
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        global_std = np.where(global_std == 0, 1.0, global_std)
        
        normalization_stats = {
            'mean': global_mean,
            'std': global_std,
            'n_samples': len(all_data)
        }
        
        # metadata í´ë”ì— global_norm.npzë¡œ ì €ì¥
        norm_file = self.metadata_dir / 'global_norm.npz'
        np.savez(norm_file, **normalization_stats)
        
        logger.info(f"Global normalization calculated from {len(all_data)} samples")
        logger.info(f"Mean: {global_mean}")
        logger.info(f"Std: {global_std}")
        logger.info(f"Normalization stats saved to: {norm_file}")
        
        return normalization_stats

    def save_file_index(self):
        """
        íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ CSVë¡œ ì €ì¥
        """
        if not self.file_index:
            logger.warning("No file index data to save")
            return
        
        df = pd.DataFrame(self.file_index)
        index_file = self.metadata_dir / 'file_index.csv'
        df.to_csv(index_file, index=False)
        logger.info(f"File index saved: {len(df)} entries to {index_file}")

    def generate_cross_validation_splits(self):
        """
        Subject-wise LOSO 5-Fold êµì°¨ê²€ì¦ split ìƒì„±
        ê° subjectë¥¼ í•œ ë²ˆì”© validation setìœ¼ë¡œ ì‚¬ìš©
        """
        if not self.file_index:
            logger.warning("No file index available for cross-validation")
            return None
        
        df = pd.DataFrame(self.file_index)
        
        # ê³ ìœ í•œ subject ëª©ë¡ ì¶”ì¶œ
        unique_subjects = sorted(df['subject'].unique())
        logger.info(f"Found {len(unique_subjects)} unique subjects: {unique_subjects}")
        
        # 5ëª…ì´ ì•„ë‹Œ ê²½ìš° ê²½ê³ 
        if len(unique_subjects) != 5:
            logger.warning(f"Expected 5 subjects for 5-Fold CV, but found {len(unique_subjects)}")
        
        # ê° foldë³„ train/validation split ìƒì„±
        cv_splits = []
        for i, test_subject in enumerate(unique_subjects):
            # í˜„ì¬ subjectëŠ” validation, ë‚˜ë¨¸ì§€ëŠ” training
            train_mask = df['subject'] != test_subject
            val_mask = df['subject'] == test_subject
            
            train_files = df[train_mask]['file_name'].tolist()
            val_files = df[val_mask]['file_name'].tolist()
            
            fold_info = {
                'fold': i + 1,
                'test_subject': test_subject,
                'train_subjects': [s for s in unique_subjects if s != test_subject],
                'train_files': train_files,
                'val_files': val_files,
                'n_train': len(train_files),
                'n_val': len(val_files)
            }
            
            cv_splits.append(fold_info)
            
            logger.info(f"Fold {i+1}: Test subject {test_subject} "
                       f"({len(val_files)} sessions), "
                       f"Train subjects {fold_info['train_subjects']} "
                       f"({len(train_files)} sessions)")
        
        # CV splitsë¥¼ JSONìœ¼ë¡œ ì €ì¥
        cv_file = self.metadata_dir / 'cv_splits.json'
        with open(cv_file, 'w', encoding='utf-8') as f:
            json.dump(cv_splits, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Cross-validation splits saved to {cv_file}")
        return cv_splits

    def analyze_data_distribution(self):
        """
        ë°ì´í„° ë¶„í¬ ë¶„ì„ (subjectë³„, taskë³„)
        """
        if not self.file_index:
            logger.warning("No file index available for analysis")
            return None
        
        df = pd.DataFrame(self.file_index)
        
        print("\n" + "="*60)
        print("DATA DISTRIBUTION ANALYSIS")
        print("="*60)
        
        # Subjectë³„ ë¶„í¬
        subject_stats = df.groupby('subject').agg({
            'file_name': 'count',
            'n_cycles': ['sum', 'mean']
        }).round(2)
        subject_stats.columns = ['n_sessions', 'total_cycles', 'avg_cycles_per_session']
        print("\nğŸ“Š Subjectë³„ ë¶„í¬:")
        print(subject_stats)
        
        # Taskë³„ ë¶„í¬
        task_stats = df.groupby('task').agg({
            'file_name': 'count',
            'n_cycles': ['sum', 'mean']
        }).round(2)
        task_stats.columns = ['n_sessions', 'total_cycles', 'avg_cycles_per_session']
        print("\nğŸ“Š Taskë³„ ë¶„í¬:")
        print(task_stats)
        
        # Subject-Task ì¡°í•©ë³„ ë¶„í¬
        print("\nğŸ“Š Subject-Task ì¡°í•©ë³„ ì„¸ì…˜ ìˆ˜:")
        pivot_table = df.pivot_table(
            values='file_name', 
            index='subject', 
            columns='task', 
            aggfunc='count',
            fill_value=0
        )
        print(pivot_table)
        
        # ì „ì²´ í†µê³„
        total_sessions = len(df)
        total_cycles = df['n_cycles'].sum()
        avg_cycles = df['n_cycles'].mean()
        
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"- ì´ ì„¸ì…˜ ìˆ˜: {total_sessions:,}")
        print(f"- ì´ ì‚¬ì´í´ ìˆ˜: {total_cycles:,}")
        print(f"- ì„¸ì…˜ë‹¹ í‰ê·  ì‚¬ì´í´: {avg_cycles:.1f}")
        print("="*60)
        
        return {
            'subject_stats': subject_stats,
            'task_stats': task_stats,
            'pivot_table': pivot_table,
            'total_stats': {
                'total_sessions': total_sessions,
                'total_cycles': total_cycles,
                'avg_cycles': avg_cycles
            }
        }

    def process_all(self):
        """
        ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        logger.info("Starting stride data processing pipeline...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±
        self.output_dir.mkdir(exist_ok=True)
        self.metadata_dir.mkdir(exist_ok=True)
        
        logger.info(f"Output directories created:")
        logger.info(f"  PKL files: {self.output_dir}")
        logger.info(f"  Metadata: {self.metadata_dir}")
        
        # ëª¨ë“  ì„¸ì…˜ í´ë” ì°¾ê¸°
        session_dirs = [d for d in self.input_dir.iterdir() if d.is_dir()]
        session_dirs.sort()  # ì •ë ¬
        
        self.processed_stats['total_sessions'] = len(session_dirs)
        logger.info(f"Found {len(session_dirs)} session directories")
        
        # ê° ì„¸ì…˜ ì²˜ë¦¬
        valid_sessions = 0
        for session_path in tqdm(session_dirs, desc="Processing sessions"):
            result = self.process_session(session_path)
            if result is not None:
                valid_sessions += 1
        
        self.processed_stats['valid_sessions'] = valid_sessions
        
        # ì „ì—­ ì •ê·œí™” í†µê³„ ê³„ì‚°
        normalization_stats = self.calculate_global_normalization()
        
        # íŒŒì¼ ì¸ë±ìŠ¤ ì €ì¥
        self.save_file_index()
        
        # ë°ì´í„° ë¶„í¬ ë¶„ì„
        distribution_stats = self.analyze_data_distribution()
        
        # êµì°¨ê²€ì¦ splits ìƒì„±
        cv_splits = self.generate_cross_validation_splits()
        
        # ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
        self.print_summary()
        
        return {
            'stats': self.processed_stats,
            'normalization': normalization_stats,
            'file_index_size': len(self.file_index),
            'distribution': distribution_stats,
            'cv_splits': cv_splits
        }

    def print_summary(self):
        """
        ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        """
        stats = self.processed_stats
        
        print("\n" + "="*60)
        print("STRIDE DATA PROCESSING SUMMARY")
        print("="*60)
        print(f"Total Sessions Found    : {stats['total_sessions']:,}")
        print(f"Valid Sessions Processed: {stats['valid_sessions']:,}")
        print(f"Empty Sessions (filtered): {stats['empty_sessions']:,}")
        print(f"")
        print(f"Total Cycles Found      : {stats['total_cycles']:,}")
        print(f"Valid Cycles Kept       : {stats['valid_cycles']:,}")
        print(f"Filtered Cycles Removed : {stats['filtered_cycles']:,}")
        print(f"")
        print(f"PKL Files Created       : {stats['valid_sessions']:,}")
        print(f"File Index Entries      : {len(self.file_index):,}")
        print(f"Global Normalization    : {'âœ“' if self.all_sequences else 'âœ—'}")
        print(f"Cross-Validation Splits : {'âœ“' if self.file_index else 'âœ—'}")
        print("="*60)


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    processor = StrideDataProcessor()
    results = processor.process_all()
    
    print(f"\nProcessing completed successfully!")
    print(f"ğŸ“ PKL files: 'stride_train_data_pkl/' folder")
    print(f"ğŸ“Š Metadata files: 'metadata/' folder")
    print(f"  - file_index.csv: íŒŒì¼ ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„°")
    print(f"  - global_norm.npz: ì „ì—­ ì •ê·œí™” í†µê³„")
    print(f"  - cv_splits.json: êµì°¨ê²€ì¦ split ì •ë³´")


if __name__ == "__main__":
    main() 