#!/usr/bin/env python3
"""
TCN (Temporal Convolutional Network) ê¸°ë°˜ ë³´í­ ì˜ˆì¸¡ ëª¨ë¸

ì•„í‚¤í…ì²˜:
- ë‘ ê°ˆë˜ ì…ë ¥: (67Ã—6) IMU ì‹œí€€ìŠ¤ + (3,) ë³´ì¡° ë²¡í„°
- Masking(mask_value=0.0) ë ˆì´ì–´ë¡œ íŒ¨ë”© ë¬´ì‹œ
- Dilated Conv1D (dilation 1,2,4,8) Ã— 4ìŠ¤íƒ
- ê° Conv: 64í•„í„°, ì»¤ë„3, causal íŒ¨ë”©
- LayerNorm + ReLU + Residual ì—°ê²° + Dropout(0.1)
- GlobalAveragePooling1D â†’ 128ì°¨ì› ì„ë² ë”©
- ë³´ì¡°ë²¡í„°ì™€ concat â†’ 131ì°¨ì›
- Dense(64) + ReLU â†’ Dense(1) ìµœì¢… ì¶œë ¥
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from typing import Dict, Optional, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@keras.utils.register_keras_serializable()
class DilatedConvBlock(layers.Layer):
    """
    Dilated Convolution ë¸”ë¡
    Conv1D â†’ LayerNorm â†’ ReLU â†’ Dropout
    """
    
    def __init__(self, filters: int, kernel_size: int = 3, dilation_rate: int = 1, 
                 dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        self.dilation_rate = dilation_rate
        self.dropout_rate = dropout_rate
        
        # Conv1D with causal padding
        self.conv = layers.Conv1D(
            filters=filters,
            kernel_size=kernel_size,
            dilation_rate=dilation_rate,
            padding='causal',
            activation=None,
            name=f'conv1d_dilation_{dilation_rate}'
        )
        
        # Layer Normalization
        self.layer_norm = layers.LayerNormalization(name=f'layer_norm_dilation_{dilation_rate}')
        
        # ReLU activation
        self.relu = layers.ReLU(name=f'relu_dilation_{dilation_rate}')
        
        # Dropout
        self.dropout = layers.Dropout(dropout_rate, name=f'dropout_dilation_{dilation_rate}')
    
    def call(self, inputs, training=None):
        x = self.conv(inputs)
        x = self.layer_norm(x)
        x = self.relu(x)
        x = self.dropout(x, training=training)
        return x
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'dilation_rate': self.dilation_rate,
            'dropout_rate': self.dropout_rate
        })
        return config

@keras.utils.register_keras_serializable()
class TCNStack(layers.Layer):
    """
    TCN ìŠ¤íƒ: Dilated Conv (1,2,4,8) + Residual ì—°ê²°
    """
    
    def __init__(self, filters: int = 64, dropout_rate: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.dropout_rate = dropout_rate
        
        # Dilated Conv ë¸”ë¡ë“¤ (dilation 1, 2, 4, 8)
        self.conv_blocks = [
            DilatedConvBlock(filters, dilation_rate=1, dropout_rate=dropout_rate),
            DilatedConvBlock(filters, dilation_rate=2, dropout_rate=dropout_rate),
            DilatedConvBlock(filters, dilation_rate=4, dropout_rate=dropout_rate),
            DilatedConvBlock(filters, dilation_rate=8, dropout_rate=dropout_rate)
        ]
        
        # Residual ì—°ê²°ì„ ìœ„í•œ 1x1 Conv (ì±„ë„ ìˆ˜ ë§ì¶”ê¸°)
        self.residual_conv = layers.Conv1D(
            filters=filters,
            kernel_size=1,
            padding='same',
            activation=None,
            name='residual_conv'
        )
        
        # ìµœì¢… Dropout
        self.final_dropout = layers.Dropout(dropout_rate, name='stack_dropout')
    
    def call(self, inputs, training=None):
        # ì…ë ¥ì„ residual ì—°ê²°ìš©ìœ¼ë¡œ ì €ì¥
        residual = self.residual_conv(inputs)
        
        # Dilated Conv ë¸”ë¡ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼
        x = inputs
        for conv_block in self.conv_blocks:
            x = conv_block(x, training=training)
        
        # Residual ì—°ê²°
        x = x + residual
        
        # ìµœì¢… Dropout
        x = self.final_dropout(x, training=training)
        
        return x
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'filters': self.filters,
            'dropout_rate': self.dropout_rate
        })
        return config

class StrideTCNModel:
    """
    TCN ê¸°ë°˜ ë³´í­ ì˜ˆì¸¡ ëª¨ë¸
    """
    
    def __init__(self, 
                 sequence_length: int = 67,
                 sequence_features: int = 6,
                 auxiliary_features: int = 3,
                 tcn_filters: int = 64,
                 tcn_stacks: int = 4,
                 dropout_rate: float = 0.1,
                 dense_units: int = 64,
                 **kwargs):
        """
        TCN ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 67)
            sequence_features: ì‹œí€€ìŠ¤ íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ê°’: 6, IMU 6ì¶•)
            auxiliary_features: ë³´ì¡° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ê°’: 3, stride_time/height/foot_id)
            tcn_filters: TCN í•„í„° ìˆ˜ (ê¸°ë³¸ê°’: 64)
            tcn_stacks: TCN ìŠ¤íƒ ìˆ˜ (ê¸°ë³¸ê°’: 4)
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)
            dense_units: Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜ (ê¸°ë³¸ê°’: 64)
        """
        self.sequence_length = sequence_length
        self.sequence_features = sequence_features
        self.auxiliary_features = auxiliary_features
        self.tcn_filters = tcn_filters
        self.tcn_stacks = tcn_stacks
        self.dropout_rate = dropout_rate
        self.dense_units = dense_units
        
        self.model = None
        
    def build_model(self) -> keras.Model:
        """ëª¨ë¸ êµ¬ì¶•"""
        
        # ì…ë ¥ ë ˆì´ì–´ë“¤
        sequence_input = layers.Input(
            shape=(self.sequence_length, self.sequence_features),
            name='sequence_input'
        )
        
        auxiliary_input = layers.Input(
            shape=(self.auxiliary_features,),
            name='auxiliary_input'
        )
        
        # ì‹œí€€ìŠ¤ ì²˜ë¦¬ ë¸Œëœì¹˜
        # Masking ë ˆì´ì–´ (0-íŒ¨ë”© ë¬´ì‹œ)
        x = layers.Masking(mask_value=0.0, name='masking')(sequence_input)
        
        # TCN ìŠ¤íƒë“¤
        for i in range(self.tcn_stacks):
            x = TCNStack(
                filters=self.tcn_filters,
                dropout_rate=self.dropout_rate,
                name=f'tcn_stack_{i+1}'
            )(x)
        
        # Global Average Pooling (ë§ˆìŠ¤í‚¹ ì •ë³´ ìœ ì§€)
        sequence_embedding = layers.GlobalAveragePooling1D(
            name='global_avg_pooling'
        )(x)
        
        # íŠ¹ì„± ìœµí•©
        # ì‹œí€€ìŠ¤ ì„ë² ë”©(128ì°¨ì›)ê³¼ ë³´ì¡° íŠ¹ì„±(3ì°¨ì›) ì—°ê²° â†’ 131ì°¨ì›
        fused_features = layers.Concatenate(
            name='feature_fusion'
        )([sequence_embedding, auxiliary_input])
        
        # ìµœì¢… ì˜ˆì¸¡ ë ˆì´ì–´ë“¤
        x = layers.Dense(
            self.dense_units,
            activation='relu',
            name='dense_hidden'
        )(fused_features)
        
        x = layers.Dropout(
            self.dropout_rate,
            name='final_dropout'
        )(x)
        
        # ì¶œë ¥ ë ˆì´ì–´ (ë³´í­ ì˜ˆì¸¡)
        output = layers.Dense(
            1,
            activation='linear',
            name='stride_length_output'
        )(x)
        
        # ëª¨ë¸ ìƒì„±
        model = keras.Model(
            inputs=[sequence_input, auxiliary_input],
            outputs=output,
            name='StrideTCNModel'
        )
        
        self.model = model
        return model
    
    def compile_model(self, 
                     learning_rate: float = 1e-3,
                     loss: str = 'mae',
                     metrics: Optional[list] = None):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        
        if self.model is None:
            raise ValueError("Model not built yet. Call build_model() first.")
        
        if metrics is None:
            metrics = ['mae', 'mse']
        
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        
        self.model.compile(
            optimizer=optimizer,
            loss=loss,
            metrics=metrics
        )
        
        logger.info(f"Model compiled with lr={learning_rate}, loss={loss}")
        
    def get_model(self) -> keras.Model:
        """ì»´íŒŒì¼ëœ ëª¨ë¸ ë°˜í™˜"""
        if self.model is None:
            raise ValueError("Model not built yet. Call build_model() first.")
        return self.model
    
    def summary(self):
        """ëª¨ë¸ ìš”ì•½ ì¶œë ¥"""
        if self.model is None:
            raise ValueError("Model not built yet. Call build_model() first.")
        return self.model.summary()
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("Model not built yet. Call build_model() first.")
        self.model.save(filepath)
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        self.model = keras.models.load_model(filepath)
        logger.info(f"Model loaded from {filepath}")
        return self.model

def create_tcn_model(sequence_length: int = 67,
                     sequence_features: int = 6,
                     auxiliary_features: int = 3,
                     tcn_filters: int = 64,
                     tcn_stacks: int = 4,
                     dropout_rate: float = 0.1,
                     dense_units: int = 64,
                     learning_rate: float = 1e-3,
                     compile_model: bool = True) -> keras.Model:
    """
    TCN ëª¨ë¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´
        sequence_features: ì‹œí€€ìŠ¤ íŠ¹ì„± ìˆ˜
        auxiliary_features: ë³´ì¡° íŠ¹ì„± ìˆ˜
        tcn_filters: TCN í•„í„° ìˆ˜
        tcn_stacks: TCN ìŠ¤íƒ ìˆ˜
        dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        dense_units: Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        compile_model: ëª¨ë¸ ì»´íŒŒì¼ ì—¬ë¶€
        
    Returns:
        keras.Model: ìƒì„±ëœ TCN ëª¨ë¸
    """
    
    tcn_model = StrideTCNModel(
        sequence_length=sequence_length,
        sequence_features=sequence_features,
        auxiliary_features=auxiliary_features,
        tcn_filters=tcn_filters,
        tcn_stacks=tcn_stacks,
        dropout_rate=dropout_rate,
        dense_units=dense_units
    )
    
    model = tcn_model.build_model()
    
    if compile_model:
        tcn_model.compile_model(learning_rate=learning_rate)
    
    return model

def get_model_callbacks(patience_early: int = 10,
                       patience_lr: int = 5,
                       lr_factor: float = 0.5,
                       min_lr: float = 1e-6,
                       monitor: str = 'val_loss',
                       save_best_path: Optional[str] = None) -> list:
    """
    í•™ìŠµìš© ì½œë°± í•¨ìˆ˜ë“¤ ìƒì„±
    
    Args:
        patience_early: EarlyStopping patience
        patience_lr: ReduceLROnPlateau patience
        lr_factor: í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨
        min_lr: ìµœì†Œ í•™ìŠµë¥ 
        monitor: ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­
        save_best_path: ìµœì  ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        
    Returns:
        list: ì½œë°± í•¨ìˆ˜ë“¤
    """
    
    callbacks = []
    
    # Early Stopping
    early_stopping = keras.callbacks.EarlyStopping(
        monitor=monitor,
        patience=patience_early,
        restore_best_weights=True,
        verbose=1
    )
    callbacks.append(early_stopping)
    
    # Reduce Learning Rate on Plateau
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor=monitor,
        factor=lr_factor,
        patience=patience_lr,
        min_lr=min_lr,
        verbose=1
    )
    callbacks.append(reduce_lr)
    
    # Model Checkpoint (ì„ íƒì )
    if save_best_path:
        checkpoint = keras.callbacks.ModelCheckpoint(
            filepath=save_best_path,
            monitor=monitor,
            save_best_only=True,
            verbose=1
        )
        callbacks.append(checkpoint)
    
    return callbacks

# í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ
if __name__ == "__main__":
    print("=== TCN ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    # ëª¨ë¸ ìƒì„±
    model = create_tcn_model(
        tcn_filters=64,
        tcn_stacks=4,
        dropout_rate=0.1,
        learning_rate=1e-3
    )
    
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ ìš”ì•½:")
    model.summary()
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    print("\n=== ë”ë¯¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ ===")
    batch_size = 4
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„±
    dummy_sequences = np.random.randn(batch_size, 67, 6).astype(np.float32)
    dummy_auxiliary = np.random.randn(batch_size, 3).astype(np.float32)
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    predictions = model.predict([dummy_sequences, dummy_auxiliary], verbose=0)
    
    print(f"âœ… ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"   ì…ë ¥ ì‹œí€€ìŠ¤ shape: {dummy_sequences.shape}")
    print(f"   ì…ë ¥ ë³´ì¡° íŠ¹ì„± shape: {dummy_auxiliary.shape}")
    print(f"   ì¶œë ¥ ì˜ˆì¸¡ shape: {predictions.shape}")
    print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: {predictions.min():.4f} ~ {predictions.max():.4f}")
    
    # ì½œë°± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    print("\n=== ì½œë°± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===")
    callbacks = get_model_callbacks(
        patience_early=10,
        patience_lr=5,
        save_best_path="test_best_model.keras"
    )
    
    print(f"âœ… ì½œë°± í•¨ìˆ˜ ìƒì„± ì™„ë£Œ: {len(callbacks)}ê°œ")
    for i, callback in enumerate(callbacks):
        print(f"   {i+1}. {type(callback).__name__}")
    
    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 